

<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="../_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="../_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="../_static/javascripts/modernizr.js"></script>
  
  
  
    <title>Everything You Need to Know about Using Slurm on Quest &#8212; Quest v2023.01.18 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/material.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Examples of Jobs on Quest" href="../Examples-of-Jobs-on-Quest/Examples-of-Jobs-on-Quest-updated.html" />
    <link rel="prev" title="Disconnecting from a RDSS/FSMResFiles share" href="../Disconnecting-from-a-RDSS-FSMResFiles-share/Disconnecting-from-a-RDSS-FSMResFiles-share-updated.html" />



  
   

  </head>
  <body dir=ltr
        data-md-color-primary=deep-purple data-md-color-accent=purple>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#Everything-You-Need-to-Know-about-Using-Slurm-on-Quest/Everything-You-Need-to-Know-about-Using-Slurm-on-Quest-updated" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../index.html" title="Quest v2023.01.18 documentation"
           class="md-header-nav__button md-logo">
          
            &nbsp;
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">Research Computing Services Documentation</span>
          <span class="md-header-nav__topic"> Everything You Need to Know about Using Slurm on Quest </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="../search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder="Search"
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            <a href="https://github.com/nuitrcs/examplejobs/" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    Quest Example Jobs
  </div>
</a>
          </div>
        </div>
      
      
  
  <script src="../_static/javascripts/version_dropdown.js"></script>
  <script>
    var json_loc = "../"versions.json"",
        target_loc = "../../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
  </script>
  

    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
          <li class="md-tabs__item"><a href="../index.html" class="md-tabs__link">Quest v2023.01.18 documentation</a></li>
            
            <li class="md-tabs__item"><a href="https://www.it.northwestern.edu/departments/it-services-support/research/index.html" class="md-tabs__link">RCS Home Page</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../index.html" title="Quest v2023.01.18 documentation" class="md-nav__button md-logo">
      
        <img src="../_static/" alt=" logo" width="48" height="48">
      
    </a>
    <a href="../index.html"
       title="Quest v2023.01.18 documentation">Research Computing Services Documentation</a>
  </label>
    <div class="md-nav__source">
      <a href="https://github.com/nuitrcs/examplejobs/" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    Quest Example Jobs
  </div>
</a>
    </div>
  
  

  
  <ul class="md-nav__list">
    <li class="md-nav__item">
    
    
      <a href="../Advanced-Globus-Features/Advanced-Globus-Features-updated.html" class="md-nav__link">Advanced Globus Features</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../AlphaFold-on-Quest/AlphaFold-on-Quest-updated.html" class="md-nav__link">AlphaFold on Quest</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Checking-Processor-and-Memory-Utilization-for-Jobs-on-Quest/Checking-Processor-and-Memory-Utilization-for-Jobs-on-Quest-updated.html" class="md-nav__link">Checking Processor and Memory Utilization for Jobs on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Checking-Processor-and-Memory-Utilization-for-Jobs-on-Quest/Checking-Processor-and-Memory-Utilization-for-Jobs-on-Quest-updated.html#how-to-check-resource-utilization-for-completed-jobs" class="md-nav__link">How to check resource utilization for completed jobs?</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Checking-Processor-and-Memory-Utilization-for-Jobs-on-Quest/Checking-Processor-and-Memory-Utilization-for-Jobs-on-Quest-updated.html#profiling-your-processes-for-memory-and-cpu-usage-before-production" class="md-nav__link">Profiling your processes for memory and CPU usage before production</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Checking-Processor-and-Memory-Utilization-for-Jobs-on-Quest/Checking-Processor-and-Memory-Utilization-for-Jobs-on-Quest-updated.html#how-to-check-resource-utilization-for-running-jobs" class="md-nav__link">How to check resource utilization for running jobs?</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Compiling-Code-on-Quest/Compiling-Code-on-Quest-updated.html" class="md-nav__link">Compiling Code on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Compiling-Code-on-Quest/Compiling-Code-on-Quest-updated.html#basic-compiling-strategies" class="md-nav__link">Basic Compiling Strategies</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Compiling-Code-on-Quest/Compiling-Code-on-Quest-updated.html#examples" class="md-nav__link">Examples</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Connecting-to-NUworkspace/Connecting-to-NUworkspace-updated.html" class="md-nav__link">Connecting to NUworkspace</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Connecting-to-NUworkspace/Connecting-to-NUworkspace-updated.html#accounts" class="md-nav__link">Accounts</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Connecting-to-NUworkspace/Connecting-to-NUworkspace-updated.html#for-support-contactnuworkspace-support-center" class="md-nav__link">For Support, contactNuworkspace Support Center</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Connecting-to-NUworkspace/Connecting-to-NUworkspace-updated.html#software-installation" class="md-nav__link">Software Installation</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Connecting-to-Quest-with-FastX/Connecting-to-Quest-with-FastX-updated.html" class="md-nav__link">Connecting to Quest with FastX</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Connecting-to-a-Research-Data-Storage-Service-Share/Connecting-to-a-Research-Data-Storage-Service-Share-updated.html" class="md-nav__link">Connecting to a Research Data Storage Service Share</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Creating-an-Amazon-S3-bucket/Creating-an-Amazon-S3-bucket-updated.html" class="md-nav__link">Creating an Amazon S3 bucket</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Debugging-your-Slurm-submission-script-on-Quest/Debugging-your-Slurm-submission-script-on-Quest-updated.html" class="md-nav__link">Debugging your Slurm submission script on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Debugging-your-Slurm-submission-script-on-Quest/Debugging-your-Slurm-submission-script-on-Quest-updated.html#debugging-a-job-submission-script-rejected-by-the-scheduler" class="md-nav__link">Debugging a Job Submission Script Rejected By The Scheduler</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Debugging-your-Slurm-submission-script-on-Quest/Debugging-your-Slurm-submission-script-on-Quest-updated.html#mistakes-that-generate-error-messages" class="md-nav__link">Mistakes That Generate Error Messages</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Debugging-your-Slurm-submission-script-on-Quest/Debugging-your-Slurm-submission-script-on-Quest-updated.html#mistakes-that-dont-generate-error-messages" class="md-nav__link">Mistakes That Don’t Generate Error Messages</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Disconnecting-from-a-RDSS-FSMResFiles-share/Disconnecting-from-a-RDSS-FSMResFiles-share-updated.html" class="md-nav__link">Disconnecting from a RDSS/FSMResFiles share</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    <label class="md-nav__link md-nav__link--active" for="__toc"> Everything You Need to Know about Using Slurm on Quest </label>
    
      <a href="#" class="md-nav__link md-nav__link--active">Everything You Need to Know about Using Slurm on Quest</a>
      
        
<nav class="md-nav md-nav--secondary">
  <ul class="md-nav__list" data-md-scrollfix="">
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="../_sources/Everything-You-Need-to-Know-about-Using-Slurm-on-Quest/Everything-You-Need-to-Know-about-Using-Slurm-on-Quest-updated.rst.txt">Show Source</a> </li>

  </ul>
</nav>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Examples-of-Jobs-on-Quest/Examples-of-Jobs-on-Quest-updated.html" class="md-nav__link">Examples of Jobs on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Examples-of-Jobs-on-Quest/Examples-of-Jobs-on-Quest-updated.html#interactive-jobs" class="md-nav__link">Interactive Jobs</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Examples-of-Jobs-on-Quest/Examples-of-Jobs-on-Quest-updated.html#batch-jobs" class="md-nav__link">Batch Jobs</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Extended-SLURM-Job-and-Submission-Options/Extended-SLURM-Job-and-Submission-Options-updated.html" class="md-nav__link">Extended SLURM Job and Submission Options</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Extended-SLURM-Job-and-Submission-Options/Extended-SLURM-Job-and-Submission-Options-updated.html#job-submission-options" class="md-nav__link">Job Submission Options</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Extended-SLURM-Job-and-Submission-Options/Extended-SLURM-Job-and-Submission-Options-updated.html#common-job-commands" class="md-nav__link">Common Job Commands</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Extended-SLURM-Job-and-Submission-Options/Extended-SLURM-Job-and-Submission-Options-updated.html#script-variables" class="md-nav__link">Script Variables</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Factors-Affecting-Job-Scheduling-on-Quest/Factors-Affecting-Job-Scheduling-on-Quest-updated.html" class="md-nav__link">Factors Affecting Job Scheduling on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Factors-Affecting-Job-Scheduling-on-Quest/Factors-Affecting-Job-Scheduling-on-Quest-updated.html#priority" class="md-nav__link">Priority</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Factors-Affecting-Job-Scheduling-on-Quest/Factors-Affecting-Job-Scheduling-on-Quest-updated.html#backfill-scheduling" class="md-nav__link">Backfill Scheduling</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../File-Systems-and-Storage-on-Quest/File-Systems-and-Storage-on-Quest-updated.html" class="md-nav__link">File Systems, Storage, and Permissions on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../File-Systems-and-Storage-on-Quest/File-Systems-and-Storage-on-Quest-updated.html#summary-of-file-permissions" class="md-nav__link">Summary of File Permissions</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../File-Systems-and-Storage-on-Quest/File-Systems-and-Storage-on-Quest-updated.html#home-directories" class="md-nav__link">Home Directories</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../File-Systems-and-Storage-on-Quest/File-Systems-and-Storage-on-Quest-updated.html#project-space" class="md-nav__link">Project Space</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../File-Systems-and-Storage-on-Quest/File-Systems-and-Storage-on-Quest-updated.html#scratch-space" class="md-nav__link">Scratch Space</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Finding-the-full-path-to-your-RDSS-share/Finding-the-full-path-to-your-RDSS-share-updated.html" class="md-nav__link">Finding the full path to your RDSS share</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Finding-the-full-path-to-your-RDSS-share/Finding-the-full-path-to-your-RDSS-share-updated.html#step-1-identify-your-access-zone" class="md-nav__link">Step 1: Identify your access zone</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Finding-the-full-path-to-your-RDSS-share/Finding-the-full-path-to-your-RDSS-share-updated.html#step-2-identify-the-full-path-to-your-share" class="md-nav__link">Step 2: Identify the full path to your share</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Finding-the-full-path-to-your-RDSS-share/Finding-the-full-path-to-your-RDSS-share-updated.html#multiple-shares" class="md-nav__link">Multiple Shares</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../GPUs-on-QUEST/GPUs-on-QUEST-updated.html" class="md-nav__link">GPUs on QUEST</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../GPUs-on-QUEST/GPUs-on-QUEST-updated.html#what-gpus-are-available-on-quest" class="md-nav__link">What GPUs are available on QUEST?</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../GPUs-on-QUEST/GPUs-on-QUEST-updated.html#interactive-gpu-jobs" class="md-nav__link">Interactive GPU jobs</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../GPUs-on-QUEST/GPUs-on-QUEST-updated.html#what-gpu-software-is-available-on-quest" class="md-nav__link">What GPU software is available on QUEST?</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Generating-a-AWS-IAM-Access-Key-for-Amazon-S3/Generating-a-AWS-IAM-Access-Key-for-Amazon-S3-updated.html" class="md-nav__link">Generating a AWS IAM Access Key for Amazon S3</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Generating-a-AWS-IAM-Access-Key-for-Amazon-S3/Generating-a-AWS-IAM-Access-Key-for-Amazon-S3-updated.html#using-the-aws-management-console" class="md-nav__link">Using the AWS Management Console</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Generating-a-AWS-IAM-Access-Key-for-Amazon-S3/Generating-a-AWS-IAM-Access-Key-for-Amazon-S3-updated.html#creating-an-iam-user" class="md-nav__link">Creating an IAM user</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Genomic-Data-Commons-Data-Transfer-Tool-on-Quest/Genomic-Data-Commons-Data-Transfer-Tool-on-Quest-updated.html" class="md-nav__link">Genomic Data Commons Data Transfer Tool on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Genomic-Data-Commons-Data-Transfer-Tool-on-Quest/Genomic-Data-Commons-Data-Transfer-Tool-on-Quest-updated.html#genomic-data-commons" class="md-nav__link">Genomic Data Commons</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Getting-Started-on-the-Genomics-Compute-Cluster-%28b1042%29-on-Quest/Getting-Started-on-the-Genomics-Compute-Cluster-%28b1042%29-on-Quest-updated.html" class="md-nav__link">Getting Started on the Genomics Compute Cluster (b1042) on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Getting-Started-on-the-Genomics-Compute-Cluster-%28b1042%29-on-Quest/Getting-Started-on-the-Genomics-Compute-Cluster-%28b1042%29-on-Quest-updated.html#about-the-genomics-compute-cluster-on-quest" class="md-nav__link">About the Genomics Compute Cluster on Quest</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Globus-Endpoints-and-Collections/Globus-Endpoints-and-Collections-updated.html" class="md-nav__link">Globus Endpoints and Collections</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Installing-Software-on-Quest/Installing-Software-on-Quest-updated.html" class="md-nav__link">Installing Software on Quest</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Installing-and-Managing-R-Packages-on-Quest/Installing-and-Managing-R-Packages-on-Quest-updated.html" class="md-nav__link">Installing and Managing R Packages on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Installing-and-Managing-R-Packages-on-Quest/Installing-and-Managing-R-Packages-on-Quest-updated.html#installing-and-managing-r-packages-on-quest-1" class="md-nav__link">Installing and Managing R Packages on Quest</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Knitro-on-Quest/Knitro-on-Quest-updated.html" class="md-nav__link">Knitro on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Knitro-on-Quest/Knitro-on-Quest-updated.html#knitro-overview" class="md-nav__link">Knitro Overview</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Knitro-on-Quest/Knitro-on-Quest-updated.html#matlab" class="md-nav__link">MATLAB</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Knitro-on-Quest/Knitro-on-Quest-updated.html#r" class="md-nav__link">R</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Knitro-on-Quest/Knitro-on-Quest-updated.html#python" class="md-nav__link">Python</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Knitro-on-Quest/Knitro-on-Quest-updated.html#ampl" class="md-nav__link">AMPL</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Logging-in-to-Quest/Logging-in-to-Quest-updated.html" class="md-nav__link">Logging in to Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Logging-in-to-Quest/Logging-in-to-Quest-updated.html#connecting-with-fastx" class="md-nav__link">Connecting with FastX</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Logging-in-to-Quest/Logging-in-to-Quest-updated.html#connecting-with-an-ssh-terminal" class="md-nav__link">Connecting with an SSH Terminal</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Managing-Full-Access-Allocations-on-Quest/Managing-Full-Access-Allocations-on-Quest-updated.html" class="md-nav__link">Managing Full Access Allocations on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Managing-Full-Access-Allocations-on-Quest/Managing-Full-Access-Allocations-on-Quest-updated.html#list-allocation-members" class="md-nav__link">List Allocation Members</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Managing-Full-Access-Allocations-on-Quest/Managing-Full-Access-Allocations-on-Quest-updated.html#status-of-allocation-jobs-in-queue" class="md-nav__link">Status of Allocation Jobs in Queue</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Managing-Full-Access-Allocations-on-Quest/Managing-Full-Access-Allocations-on-Quest-updated.html#status-of-allocation-nodes" class="md-nav__link">Status of Allocation Nodes</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Managing-Full-Access-Allocations-on-Quest/Managing-Full-Access-Allocations-on-Quest-updated.html#individual-node-status" class="md-nav__link">Individual Node Status</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Managing-Full-Access-Allocations-on-Quest/Managing-Full-Access-Allocations-on-Quest-updated.html#why-are-my-jobs-running-on-nodes-other-than-those-reserved-for-my-allocation" class="md-nav__link">Why are my jobs running on nodes other than those reserved for my allocation?</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Managing-Jobs-on-Quest/Managing-Jobs-on-Quest-updated.html" class="md-nav__link">Managing Jobs on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Managing-Jobs-on-Quest/Managing-Jobs-on-Quest-updated.html#job-id" class="md-nav__link">Job ID</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Managing-Jobs-on-Quest/Managing-Jobs-on-Quest-updated.html#job-status" class="md-nav__link">Job Status</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Managing-Jobs-on-Quest/Managing-Jobs-on-Quest-updated.html#commonly-used-commands" class="md-nav__link">Commonly Used Commands</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Managing-access-to-Research-Data-Storage-Service-%28RDSS%29-Shares/Managing-access-to-Research-Data-Storage-Service-%28RDSS%29-Shares-updated.html" class="md-nav__link">Managing access to Research Data Storage Service (RDSS) Shares</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Managing-an-Allocation-on-Quest/Managing-an-Allocation-on-Quest-updated.html" class="md-nav__link">Managing an Allocation on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Managing-an-Allocation-on-Quest/Managing-an-Allocation-on-Quest-updated.html#allocation-resource-management" class="md-nav__link">Allocation Resource Management</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Managing-an-Allocation-on-Quest/Managing-an-Allocation-on-Quest-updated.html#list-allocation-members" class="md-nav__link">List Allocation Members</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Managing-an-Allocation-on-Quest/Managing-an-Allocation-on-Quest-updated.html#check-allocation-resources" class="md-nav__link">Check Allocation Resources</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Modules-Software-Environment-Manager-on-Quest/Modules-Software-Environment-Manager-on-Quest-updated.html" class="md-nav__link">Modules Software Environment Manager on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Modules-Software-Environment-Manager-on-Quest/Modules-Software-Environment-Manager-on-Quest-updated.html#available-modules" class="md-nav__link">Available Modules</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Modules-Software-Environment-Manager-on-Quest/Modules-Software-Environment-Manager-on-Quest-updated.html#loading-a-module" class="md-nav__link">Loading a Module</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Modules-Software-Environment-Manager-on-Quest/Modules-Software-Environment-Manager-on-Quest-updated.html#module-versions" class="md-nav__link">Module Versions</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Modules-Software-Environment-Manager-on-Quest/Modules-Software-Environment-Manager-on-Quest-updated.html#useful-module-commands" class="md-nav__link">Useful Module Commands</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Opening-an-Archived-File-in-RDSS/Opening-an-Archived-File-in-RDSS-updated.html" class="md-nav__link">Opening an Archived File in RDSS</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Parabricks-on-the-Genomics-Compute-Cluster/Parabricks-on-the-Genomics-Compute-Cluster-updated.html" class="md-nav__link">Parabricks on the Genomics Compute Cluster</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Parabricks-on-the-Genomics-Compute-Cluster/Parabricks-on-the-Genomics-Compute-Cluster-updated.html#checking-out-parabricks-licenses" class="md-nav__link">Checking out Parabricks Licenses</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Parabricks-on-the-Genomics-Compute-Cluster/Parabricks-on-the-Genomics-Compute-Cluster-updated.html#running-parabricks-on-quest" class="md-nav__link">Running Parabricks on Quest</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Quest-FAQ/Quest-FAQ-updated.html" class="md-nav__link">Quest FAQ</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Quest-Partitions-Queues/Quest-Partitions-Queues-updated.html" class="md-nav__link">Quest Partitions/Queues</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Quest-Partitions-Queues/Quest-Partitions-Queues-updated.html#partition-definitions-general-access-p-and-e-accounts" class="md-nav__link">Partition Definitions: General Access (“p” and “e” accounts)</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Quest-Partitions-Queues/Quest-Partitions-Queues-updated.html#partition-definitions-full-access-buy-ins-or-b-accounts" class="md-nav__link">Partition Definitions: Full Access (buy-ins, or “b” accounts)</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Quest-Partitions-Queues/Quest-Partitions-Queues-updated.html#notes" class="md-nav__link">Notes</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Quest-Quick-Start-Guide/Quest-Quick-Start-Guide-updated.html" class="md-nav__link">Quest Quick Start Guide</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Quest-Quick-Start-Guide/Quest-Quick-Start-Guide-updated.html#navigating-quest" class="md-nav__link">Navigating Quest</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Quest-Quick-Start-Guide/Quest-Quick-Start-Guide-updated.html#moving-files-onto-quest" class="md-nav__link">Moving Files onto Quest</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Quest-Quick-Start-Guide/Quest-Quick-Start-Guide-updated.html#software-available-on-quest" class="md-nav__link">Software available on Quest</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Quest-Quick-Start-Guide/Quest-Quick-Start-Guide-updated.html#running-jobs-on-quest" class="md-nav__link">Running Jobs on Quest</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Quest-Quick-Start-Guide/Quest-Quick-Start-Guide-updated.html#managing-your-jobs-on-quest" class="md-nav__link">Managing Your Jobs on Quest</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Quest-Quick-Start-Guide/Quest-Quick-Start-Guide-updated.html#getting-help" class="md-nav__link">Getting Help</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Quest-Slurm-Quick-Start/Quest-Slurm-Quick-Start-updated.html" class="md-nav__link">Quest Slurm Quick Start</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Quest-Slurm-Quick-Start/Quest-Slurm-Quick-Start-updated.html#quick-start" class="md-nav__link">Quick Start</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Quest-Troubleshooting%3A-Checking-the-job-output-file/Quest-Troubleshooting%3A-Checking-the-job-output-file-updated.html" class="md-nav__link">Quest Troubleshooting: Checking the job output file</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Quest-User-Guide/Quest-User-Guide-updated.html" class="md-nav__link">Quest User Guide</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Quest-User-Guide/Quest-User-Guide-updated.html#user-guides" class="md-nav__link">User Guides</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Research-Data-Management-Guide/Research-Data-Management-Guide-updated.html" class="md-nav__link">Research Data Management Guide</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Research-Data-Management-Guide/Research-Data-Management-Guide-updated.html#research-data-storage-service-rdss" class="md-nav__link">Research Data Storage Service (RDSS)</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Research-Data-Management-Guide/Research-Data-Management-Guide-updated.html#globus-data-transfer-tool" class="md-nav__link">Globus data transfer tool</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Resolving-issues-accessing-a-Research-Data-Storage-Service-share/Resolving-issues-accessing-a-Research-Data-Storage-Service-share-updated.html" class="md-nav__link">Resolving issues accessing a Research Data Storage Service share</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Restoring-a-previous-version-of-a-file-or-folder-from-RDSS/Restoring-a-previous-version-of-a-file-or-folder-from-RDSS-updated.html" class="md-nav__link">Restoring a previous version of a file or folder from RDSS</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Restoring-a-previous-version-of-a-file-or-folder-from-RDSS/Restoring-a-previous-version-of-a-file-or-folder-from-RDSS-updated.html#retrieving-older-versions-of-modified-deleted-files" class="md-nav__link">Retrieving older versions of modified/deleted files</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Running-GATK4-Spark-on-Quest/Running-GATK4-Spark-on-Quest-updated.html" class="md-nav__link">Running GATK4 Spark on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Running-GATK4-Spark-on-Quest/Running-GATK4-Spark-on-Quest-updated.html#gatk4-spark-tools" class="md-nav__link">GATK4 SPARK tools</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Running-GATK4-Spark-on-Quest/Running-GATK4-Spark-on-Quest-updated.html#converting-fasta-files-for-parallel-runs" class="md-nav__link">Converting FASTA Files for Parallel Runs</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Running-GATK4-Spark-on-Quest/Running-GATK4-Spark-on-Quest-updated.html#example-job-submission" class="md-nav__link">Example Job Submission</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Running-Jupyter-Notebook-on-Quest/Running-Jupyter-Notebook-on-Quest-updated.html" class="md-nav__link">Running Jupyter Notebook on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Running-Jupyter-Notebook-on-Quest/Running-Jupyter-Notebook-on-Quest-updated.html#accessing-conda-environments-in-jupyter" class="md-nav__link">Accessing Conda Environments in Jupyter</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Running-Jupyter-Notebook-on-Quest/Running-Jupyter-Notebook-on-Quest-updated.html#create-a-ipython-kernel" class="md-nav__link">Create a iPython Kernel</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Running-Jupyter-Notebook-on-Quest/Running-Jupyter-Notebook-on-Quest-updated.html#create-a-r-kernel" class="md-nav__link">Create a R Kernel</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Software-on-Quest/Software-on-Quest-updated.html" class="md-nav__link">Software on Quest</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Specifying-Memory-for-Jobs-on-Quest/Specifying-Memory-for-Jobs-on-Quest-updated.html" class="md-nav__link">Specifying Memory for Jobs on Quest</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Transferring-Files-to-and-from-Quest/Transferring-Files-to-and-from-Quest-updated.html" class="md-nav__link">Transferring Files to and from Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Transferring-Files-to-and-from-Quest/Transferring-Files-to-and-from-Quest-updated.html#globus" class="md-nav__link">Globus</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Transferring-Files-to-and-from-Quest/Transferring-Files-to-and-from-Quest-updated.html#other-data-transfer-options" class="md-nav__link">Other Data Transfer Options</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Transferring-files-using-Globus/Transferring-files-using-Globus-updated.html" class="md-nav__link">Transferring files using Globus</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Transferring-files-using-Globus/Transferring-files-using-Globus-updated.html#to-initiate-a-file-transfer" class="md-nav__link">To initiate a file transfer:</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Troubleshooting-Installing-R-Packages-on-Quest-and-Quest-Analytics/Troubleshooting-Installing-R-Packages-on-Quest-and-Quest-Analytics-updated.html" class="md-nav__link">Troubleshooting Installing R Packages on Quest and Quest Analytics</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Troubleshooting-Installing-R-Packages-on-Quest-and-Quest-Analytics/Troubleshooting-Installing-R-Packages-on-Quest-and-Quest-Analytics-updated.html#introduction" class="md-nav__link">Introduction</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Troubleshooting-Jobs-on-Quest/Troubleshooting-Jobs-on-Quest-updated.html" class="md-nav__link">Troubleshooting Jobs on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Troubleshooting-Jobs-on-Quest/Troubleshooting-Jobs-on-Quest-updated.html#why-isnt-my-job-running" class="md-nav__link">Why isn’t my job running?</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Troubleshooting-Jobs-on-Quest/Troubleshooting-Jobs-on-Quest-updated.html#check-the-output-file" class="md-nav__link">Check the Output File</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Troubleshooting-Jobs-on-Quest/Troubleshooting-Jobs-on-Quest-updated.html#resources-exceeded" class="md-nav__link">Resources Exceeded</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Troubleshooting-Jobs-on-Quest/Troubleshooting-Jobs-on-Quest-updated.html#out-of-disk-space" class="md-nav__link">Out of Disk Space</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Troubleshooting-Jobs-on-Quest/Troubleshooting-Jobs-on-Quest-updated.html#examples" class="md-nav__link">Examples</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Troubleshooting-R-on-Quest/Troubleshooting-R-on-Quest-updated.html" class="md-nav__link">Troubleshooting R on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Troubleshooting-R-on-Quest/Troubleshooting-R-on-Quest-updated.html#common-issues" class="md-nav__link">Common Issues</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Amazon-S3-Storage/Using-Amazon-S3-Storage-updated.html" class="md-nav__link">Using Amazon S3 Storage</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Git-on-Quest/Using-Git-on-Quest-updated.html" class="md-nav__link">Using Git on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Using-Git-on-Quest/Using-Git-on-Quest-updated.html#making-git-available" class="md-nav__link">Making Git Available</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Git-on-Quest/Using-Git-on-Quest-updated.html#setting-up-git" class="md-nav__link">Setting up Git</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Git-on-Quest/Using-Git-on-Quest-updated.html#repositories" class="md-nav__link">Repositories</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Git-on-Quest/Using-Git-on-Quest-updated.html#git-resources" class="md-nav__link">Git Resources</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Globus-Timer/Using-Globus-Timer-updated.html" class="md-nav__link">Using Globus Timer</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Globus-with-Amazon-S3-storage/Using-Globus-with-Amazon-S3-storage-updated.html" class="md-nav__link">Using Globus with Amazon S3 storage</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Using-Globus-with-Amazon-S3-storage/Using-Globus-with-Amazon-S3-storage-updated.html#setup" class="md-nav__link">Setup</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Jupyter-on-the-Quest-Analytics-Nodes/Using-Jupyter-on-the-Quest-Analytics-Nodes-updated.html" class="md-nav__link">Using Jupyter on the Quest Analytics Nodes</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Using-Jupyter-on-the-Quest-Analytics-Nodes/Using-Jupyter-on-the-Quest-Analytics-Nodes-updated.html#overview" class="md-nav__link">Overview</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Jupyter-on-the-Quest-Analytics-Nodes/Using-Jupyter-on-the-Quest-Analytics-Nodes-updated.html#connecting" class="md-nav__link">Connecting</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Jupyter-on-the-Quest-Analytics-Nodes/Using-Jupyter-on-the-Quest-Analytics-Nodes-updated.html#transferring-and-accessing-files" class="md-nav__link">Transferring and Accessing Files</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Jupyter-on-the-Quest-Analytics-Nodes/Using-Jupyter-on-the-Quest-Analytics-Nodes-updated.html#environments-and-packages" class="md-nav__link">Environments and Packages</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Jupyter-on-the-Quest-Analytics-Nodes/Using-Jupyter-on-the-Quest-Analytics-Nodes-updated.html#system-details" class="md-nav__link">System Details</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Jupyter-on-the-Quest-Analytics-Nodes/Using-Jupyter-on-the-Quest-Analytics-Nodes-updated.html#issues-or-problems-with-the-quest-analytics-nodes" class="md-nav__link">Issues or Problems with the Quest Analytics nodes</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-MATLAB-on-Quest/Using-MATLAB-on-Quest-updated.html" class="md-nav__link">Using MATLAB on Quest</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Python-on-Quest/Using-Python-on-Quest-updated.html" class="md-nav__link">Using Python on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Using-Python-on-Quest/Using-Python-on-Quest-updated.html#availability" class="md-nav__link">Availability</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Python-on-Quest/Using-Python-on-Quest-updated.html#python-package-management" class="md-nav__link">Python Package Management</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-QIIME2-on-Quest/Using-QIIME2-on-Quest-updated.html" class="md-nav__link">Using QIIME2 on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Using-QIIME2-on-Quest/Using-QIIME2-on-Quest-updated.html#installing-qiime2-on-quest" class="md-nav__link">Installing QIIME2 on Quest</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-R-on-Quest/Using-R-on-Quest-updated.html" class="md-nav__link">Using R on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Using-R-on-Quest/Using-R-on-Quest-updated.html#availability" class="md-nav__link">Availability</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-R-on-Quest/Using-R-on-Quest-updated.html#installing-and-managing-r-packages-on-quest" class="md-nav__link">Installing and Managing R Packages on Quest</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-RStudio-on-the-Quest-Analytics-Nodes/Using-RStudio-on-the-Quest-Analytics-Nodes-updated.html" class="md-nav__link">Using RStudio on the Quest Analytics Nodes</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Using-RStudio-on-the-Quest-Analytics-Nodes/Using-RStudio-on-the-Quest-Analytics-Nodes-updated.html#overview" class="md-nav__link">Overview</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-RStudio-on-the-Quest-Analytics-Nodes/Using-RStudio-on-the-Quest-Analytics-Nodes-updated.html#connecting" class="md-nav__link">Connecting</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-RStudio-on-the-Quest-Analytics-Nodes/Using-RStudio-on-the-Quest-Analytics-Nodes-updated.html#transferring-and-accessing-files" class="md-nav__link">Transferring and Accessing Files</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-RStudio-on-the-Quest-Analytics-Nodes/Using-RStudio-on-the-Quest-Analytics-Nodes-updated.html#installing-packages" class="md-nav__link">Installing Packages</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-RStudio-on-the-Quest-Analytics-Nodes/Using-RStudio-on-the-Quest-Analytics-Nodes-updated.html#system-details" class="md-nav__link">System Details</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-RStudio-on-the-Quest-Analytics-Nodes/Using-RStudio-on-the-Quest-Analytics-Nodes-updated.html#issues-or-problems-with-the-analytics-nodes" class="md-nav__link">Issues or Problems with the Analytics Nodes</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-SAS-Studio-on-the-Quest-Analytics-Nodes/Using-SAS-Studio-on-the-Quest-Analytics-Nodes-updated.html" class="md-nav__link">Using SAS Studio on the Quest Analytics Nodes</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Using-SAS-Studio-on-the-Quest-Analytics-Nodes/Using-SAS-Studio-on-the-Quest-Analytics-Nodes-updated.html#overview" class="md-nav__link">Overview</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-SAS-Studio-on-the-Quest-Analytics-Nodes/Using-SAS-Studio-on-the-Quest-Analytics-Nodes-updated.html#connecting" class="md-nav__link">Connecting</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-SAS-Studio-on-the-Quest-Analytics-Nodes/Using-SAS-Studio-on-the-Quest-Analytics-Nodes-updated.html#transferring-and-accessing-files" class="md-nav__link">Transferring and Accessing Files</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-SAS-Studio-on-the-Quest-Analytics-Nodes/Using-SAS-Studio-on-the-Quest-Analytics-Nodes-updated.html#system-details" class="md-nav__link">System Details</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-SAS-Studio-on-the-Quest-Analytics-Nodes/Using-SAS-Studio-on-the-Quest-Analytics-Nodes-updated.html#issues-or-problems-with-the-analytics-nodes" class="md-nav__link">Issues or Problems with the Analytics Nodes</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Singularity-on-Quest/Using-Singularity-on-Quest-updated.html" class="md-nav__link">Using Singularity on Quest</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Using-Singularity-on-Quest/Using-Singularity-on-Quest-updated.html#load-the-singularity-module" class="md-nav__link">Load the Singularity Module</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Singularity-on-Quest/Using-Singularity-on-Quest-updated.html#getting-singularity-images-on-quest" class="md-nav__link">Getting Singularity Images On Quest</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Singularity-on-Quest/Using-Singularity-on-Quest-updated.html#running-singularity-containers" class="md-nav__link">Running Singularity Containers</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-Singularity-on-Quest/Using-Singularity-on-Quest-updated.html#best-practices" class="md-nav__link">Best Practices</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-Globus-Data-Transfer-Tool/Using-the-Globus-Data-Transfer-Tool-updated.html" class="md-nav__link">Using the Globus Data Transfer Tool</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Using-the-Globus-Data-Transfer-Tool/Using-the-Globus-Data-Transfer-Tool-updated.html#overview" class="md-nav__link">Overview</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-Globus-Data-Transfer-Tool/Using-the-Globus-Data-Transfer-Tool-updated.html#restrictions" class="md-nav__link">Restrictions</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-Globus-Data-Transfer-Tool/Using-the-Globus-Data-Transfer-Tool-updated.html#endpoints-and-collections" class="md-nav__link">Endpoints and Collections</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-Globus-Data-Transfer-Tool/Using-the-Globus-Data-Transfer-Tool-updated.html#transferring-data" class="md-nav__link">Transferring data</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-Globus-Data-Transfer-Tool/Using-the-Globus-Data-Transfer-Tool-updated.html#advanced-features" class="md-nav__link">Advanced features</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-Globus-Data-Transfer-Tool/Using-the-Globus-Data-Transfer-Tool-updated.html#resources" class="md-nav__link">Resources</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-OneDrive-SharePoint-Globus-Endpoint/Using-the-OneDrive-SharePoint-Globus-Endpoint-updated.html" class="md-nav__link">Using the OneDrive/SharePoint Globus Endpoint</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Using-the-OneDrive-SharePoint-Globus-Endpoint/Using-the-OneDrive-SharePoint-Globus-Endpoint-updated.html#setup" class="md-nav__link">Setup</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-OneDrive-SharePoint-Globus-Endpoint/Using-the-OneDrive-SharePoint-Globus-Endpoint-updated.html#activating-the-onedrive-sharepoint-endpoint" class="md-nav__link">Activating the OneDrive/Sharepoint Endpoint</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-OneDrive-SharePoint-Globus-Endpoint/Using-the-OneDrive-SharePoint-Globus-Endpoint-updated.html#accessing-files" class="md-nav__link">Accessing files</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-OneDrive-SharePoint-Globus-Endpoint/Using-the-OneDrive-SharePoint-Globus-Endpoint-updated.html#my-file-transfer-fails-because-of-a-checksum-error-when-i-try-to-send-a-microsoft-office-document-to-a-sharepoint-library" class="md-nav__link">My file transfer fails because of a checksum error when I try to send a Microsoft Office Document to a SharePoint library.</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-OneDrive-SharePoint-Globus-Endpoint/Using-the-OneDrive-SharePoint-Globus-Endpoint-updated.html#i-cant-see-a-sharepoint-library-i-have-access-to" class="md-nav__link">I can’t see a SharePoint Library I have access to.</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-Quest-Globus-endpoint/Using-the-Quest-Globus-endpoint-updated.html" class="md-nav__link">Using the Quest Globus endpoint</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-RDSS-Globus-Endpoint/Using-the-RDSS-Globus-Endpoint-updated.html" class="md-nav__link">Using the RDSS Globus Endpoint</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="../Using-the-RDSS-Globus-Endpoint/Using-the-RDSS-Globus-Endpoint-updated.html#setup" class="md-nav__link">Setup</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-RDSS-Globus-Endpoint/Using-the-RDSS-Globus-Endpoint-updated.html#mounting-your-rdss-fsmresfiles-share" class="md-nav__link">Mounting your RDSS/FSMResFiles share</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-RDSS-Globus-Endpoint/Using-the-RDSS-Globus-Endpoint-updated.html#transferring-files-to-from-rdss-fsmresfiles" class="md-nav__link">Transferring files to/from RDSS<strong>/FSMResFiles</strong></a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-RDSS-Globus-Endpoint/Using-the-RDSS-Globus-Endpoint-updated.html#extending-rdss-fsmresfiles-endpoint-activation" class="md-nav__link">Extending RDSS<strong>/FSMResFiles</strong> endpoint activation</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../Using-the-RDSS-Globus-Endpoint/Using-the-RDSS-Globus-Endpoint-updated.html#troubleshooting" class="md-nav__link">Troubleshooting</a>
      
    
    </li></ul>
    
    </li>
  </ul>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
  <ul class="md-nav__list" data-md-scrollfix="">
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="../_sources/Everything-You-Need-to-Know-about-Using-Slurm-on-Quest/Everything-You-Need-to-Know-about-Using-Slurm-on-Quest-updated.rst.txt">Show Source</a> </li>

<li id="searchbox" class="md-nav__item"></li>

  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  <section id="everything-you-need-to-know-about-using-slurm-on-quest">
<h1 id="everything-you-need-to-know-about-using-slurm-on-quest-everything-you-need-to-know-about-using-slurm-on-quest-updated--page-root">Everything You Need to Know about Using Slurm on Quest<a class="headerlink" href="#everything-you-need-to-know-about-using-slurm-on-quest-everything-you-need-to-know-about-using-slurm-on-quest-updated--page-root" title="Permalink to this heading">¶</a></h1>
<p>This page contains in-depth information on submitting jobs on Quest
through Slurm.</p>
<p>The program that schedules jobs and manages resources on Quest is called
Slurm. This page is designed to help facilitate a deeper understanding
of how Slurm works and to contextualize the things that you can do with
Slurm. For those who are brand new to Quest, we recommend visiting our
<a class="reference external" href="https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=2004">Research Computing How-to
Videos</a>
page and watching our <em>Introduction to Quest</em> video before proceeding
with this documentation. This page covers:</p>
<ul class="simple">
<li><p>A simple Slurm job submission script.</p></li>
<li><p>The key configuration settings of the Slurm submission script as well
as details on additional settings that may be useful to specify.</p></li>
<li><p>How to monitor and manage your job submissions</p></li>
<li><p>Special types of job submissions and when they are useful.</p></li>
<li><p>Diagnosing issues with your job submission.</p></li>
</ul>
<div class="docutils container" id="container">
<div class="docutils container" id="tableofcontents">
<ul class="simple">
<li><p><a class="reference external" href="#section-job-submission-script">The Job Submission Script</a></p>
<ul>
<li><p><a class="reference external" href="#section-example-submission-script">Example Submission
Script</a></p></li>
<li><p><a class="reference external" href="#section-submitting-your-batch-job">Submitting Your Batch
Job</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#key-slurm-configuration-settings">Slurm Configuration
Settings</a></p>
<ul>
<li><p><a class="reference external" href="#section-account">Account/Allocation</a></p></li>
<li><p><a class="reference external" href="#section-partitions">Partition/Queue</a></p></li>
<li><p><a class="reference external" href="#section-walltime">Walltime/Length of the job</a></p></li>
<li><p><a class="reference external" href="#section-number-of-nodes">Number of nodes</a></p></li>
<li><p><a class="reference external" href="#section-number-of-cores">Number of cores</a></p></li>
<li><p><a class="reference external" href="#section-required-memory">Required memory</a></p></li>
<li><p><a class="reference external" href="#section-output-error">Standard output/error</a></p></li>
<li><p><a class="reference external" href="#section-job-name">Job Name</a></p></li>
<li><p><a class="reference external" href="#section-email">E-Mail Alerts</a></p></li>
<li><p><a class="reference external" href="#section-constraints">Constraint</a></p></li>
<li><p><a class="reference external" href="#section-all-slurm-options">All Slurm Configuration
Settings</a></p></li>
<li><p><a class="reference external" href="#section-slurm-environmental-variables">Environmental Variables Set by
Slurm</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#all-slurm-commands-and-submission-options">SLURM Commands and Job
Management</a></p>
<ul>
<li><p><a class="reference external" href="#section-common-slurm-commands">Table of Common Slurm
Commands</a></p></li>
<li><p><a class="reference external" href="#squeue">squeue</a></p></li>
<li><p><a class="reference external" href="#sacct">sacct</a></p></li>
<li><p><a class="reference external" href="#seff">seff</a></p></li>
<li><p><a class="reference external" href="#checkjob">checkjob</a></p></li>
<li><p><a class="reference external" href="#scancel">scancel</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#section-special-job-types">Special Types of Job
Submissions</a></p>
<ul>
<li><p><a class="reference external" href="#section-interactive-jobs">Interactive Jobs</a></p>
<ul>
<li><p><a class="reference external" href="#section-section-interactive-jobs-non-gui">Without Graphical User
Interface</a></p></li>
<li><p><a class="reference external" href="#section-section-interactive-jobs-gui">With Graphical User
Interface</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#section-job-array">Job Array</a></p></li>
<li><p><a class="reference external" href="#section-dependent-jobs">Dependent Jobs</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#section-job-scheduling">Factors Affecting Job Scheduling on
Quest</a></p></li>
<li><p><a class="reference external" href="#section-diagnosing-jobs">Diagnosing Issues with Your Job Submission Script and/or Your
Job Itself</a></p>
<ul>
<li><p><a class="reference external" href="#section-debugging-jobs">Debugging a Job Rejected by the
Scheduler</a></p></li>
<li><p><a class="reference external" href="#h_94310085932911650907024000">Debugging a Job Accepted by the Scheduler that Fails
Immediately</a>` &lt;#section-job-scheduling&gt;`__</p></li>
</ul>
</li>
<li><p><a class="reference external" href="#section-job-failures">Common Reasons for Failed Jobs</a></p>
<ul>
<li><p><a class="reference external" href="#section-not-enough-memory-or-time">Not enough memory or time
requested</a></p></li>
<li><p><a class="reference external" href="#section-ran-out-of-disk">Ran out of disk space in home folder or allocation
folder</a></p></li>
</ul>
</li>
</ul>
<p>Expand All Collapse All</p>
</div>
<div class="docutils container" id="content">
<p class="rubric" id="section-job-submission-script">The Job Submission Script</p>
<p>Slurm requires users to write a batch submission script to run a
batch job. In this section, we present a basic submission script
and how a user would then submit a batch job using this script. In
the submission script, you specify the resources you need and what
commands to run. You submit this script to the scheduler by
running the <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> command on the command line.</p>
<p class="rubric" id="section-example-submission-script">Example Submission Script</p>
<p>The following is an example batch job submission script. Please
note any placeholders which are denoted with <code class="docutils literal notranslate"><span class="pre">&lt;&gt;</span></code>. You will need
to replace these with the appropriate value, and remove the
<code class="docutils literal notranslate"><span class="pre">&lt;&gt;</span></code>. This job submission script would be saved in a file such
as <code class="docutils literal notranslate"><span class="pre">jobscript.sh</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --account=&lt;account&gt; ## Required: your allocation/account name, i.e. eXXXX, pXXXX or bXXXX</span>
<span class="c1">#SBATCH --partition=short ## Required: (buyin, short, normal, long, gengpu, genhimem, etc)</span>
<span class="c1">#SBATCH --time=00:10:00 ## Required: How long will the job need to run (remember different partitions have restrictions on this parameter)</span>
<span class="c1">#SBATCH --nodes=1 ## how many computers/nodes do you need (no default)</span>
<span class="c1">#SBATCH --ntasks-per-node=1 ## how many cpus or processors do you need on per computer/node (default value 1)</span>
<span class="c1">#SBATCH --mem=1G ## how much RAM do you need per computer/node (this affects your FairShare score so be careful to not ask for more than you need))</span>
<span class="c1">#SBATCH --job-name=sample_job ## When you run squeue -u  this is how you can identify the job</span>
<span class="c1">#SBATCH --output=output.log ## standard out and standard error goes to this file</span>

<span class="c1"># A regular comment in Bash</span>
<span class="n">module</span> <span class="n">purge</span> <span class="nb">all</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">python</span><span class="o">-</span><span class="n">miniconda3</span><span class="o">/</span><span class="mf">4.12.0</span>
<span class="n">source</span> <span class="n">activate</span> <span class="o">/</span><span class="n">projects</span><span class="o">/</span><span class="n">intro</span><span class="o">/</span><span class="n">envs</span><span class="o">/</span><span class="n">slurm</span><span class="o">-</span><span class="n">py37</span><span class="o">-</span><span class="n">test</span>

<span class="n">python</span> <span class="o">--</span><span class="n">version</span>
<span class="n">python</span> <span class="n">slurm_test</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>The first line of the script loads the bash shell. Only the lines
that begin with <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> are interpreted by Slurm at the time
of job submission. Any words following <code class="docutils literal notranslate"><span class="pre">##</span></code> are treated as
comments and ignored by Slurm interpreter. Once Slurm places the
job on a compute node, the remainder of the script (everything
after the last <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> line) is run. Normally in Bash, <code class="docutils literal notranslate"><span class="pre">#</span></code>
is a comment character which means that anything written after a
<code class="docutils literal notranslate"><span class="pre">#</span></code> is ignored by the Bash interpreter/language. When writing a
submission script, however, the Slurm interpreter recognizes
#SBATCH as a command.</p>
<p>After the Slurm commands, the rest of the script works like a
regular Bash script. You can modify environment variables, load
modules, change directories, and execute program commands. Lines
in the second half of the script that start with <code class="docutils literal notranslate"><span class="pre">#</span></code> are
comments, such as <code class="docutils literal notranslate"><span class="pre">#</span> <span class="pre">A</span> <span class="pre">regular</span> <span class="pre">comment</span> <span class="pre">in</span> <span class="pre">Bash</span></code> in the the
example above.</p>
<p>Find a downloadable copy of this example script <a class="reference external" href="https://github.com/nuitrcs/examplejobs">on
GitHub</a>.</p>
<p class="rubric" id="section-submitting-your-batch-job">Submitting Your Batch Job</p>
<p>After you have written and saved your submission script, you can
submit your job. At the command line type</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>sbatch &lt;name_of_script&gt;
</pre></div>
</div>
<p>where, in the example above <code class="docutils literal notranslate"><span class="pre">&lt;name_of_script&gt;</span></code> would be
<code class="docutils literal notranslate"><span class="pre">jobscript.sh</span></code>. Upon submission the scheduler will return your
job number:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>Submitted batch job 549005
</pre></div>
</div>
<p>If you have a workflow that accepts or needs the jobid as an input
for <a class="reference external" href="#all-slurm-commands-and-submission-options">job
monitoring</a> or <a class="reference external" href="#section-dependent-jobs">job
dependencies</a>, then you may prefer the
return value of your job submission be just the job number. To do
this, pass the <code class="docutils literal notranslate"><span class="pre">--parsable</span></code> argument:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>sbatch --parsable &lt;name_of_script&gt;
549005
</pre></div>
</div>
<p>If there is an error in your job submission script, the job will
not be accepted by the scheduler and you will receive an error
message right away, for example:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified
</pre></div>
</div>
<p>If your job submission receives an error, you will need to address
the issue and resubmit your job. If no error is received, your job
has entered the queue and will run.</p>
<p class="rubric" id="slurm-configuration-settings">Slurm Configuration Settings</p>
<p>In this section, we go into details about a number of the Slurm
configurations settings. In each subsection, we include;</p>
<ul class="simple">
<li><p>the possible range of values a user can set,</p></li>
<li><p>how to think about what value to use for a given setting,</p></li>
<li><p>whether the setting is required, and</p></li>
<li><p>if a setting is required, what the default is for the setting.</p></li>
</ul>
<p class="rubric" id="section-account">Allocation/Account</p>
<p>To specify the account, please include a <code class="docutils literal notranslate"><span class="pre">-A/--account</span></code> option
in your job submission script:</p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-A</span> <span class="pre">&lt;allocation&gt;</span></code></p>
<p>Slurm offers two methods, one short (such as <code class="docutils literal notranslate"><span class="pre">-A</span></code>) and one
verbose (<code class="docutils literal notranslate"><span class="pre">--account</span></code>) to write most of its setting flags. For
verbose methods, an <code class="docutils literal notranslate"><span class="pre">=</span></code> sign is required after the flag (such as
<code class="docutils literal notranslate"><span class="pre">--account=</span></code>).</p>
<p>To submit jobs to Quest, you must be part of an <strong>active</strong>
classroom, research, or buy-in allocation. Additionally, for
buy-in allocations, you must be part of a <strong>buy-in allocation with
access to compute resources</strong>. To determine the names of the
allocation(s)/account(s) that you are part of on Quest, you can
run the following on the command line which will produce output
similar to below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ groups
quest_demo b1011 p30157 e31572
</pre></div>
</div>
<p>Once you determine the allocation(s)/account(s) that you are part
of, you can check whether the allocation is active and has access
to compute resources by running <code class="docutils literal notranslate"><span class="pre">checkproject</span></code> and reading the
last two lines of the output. An example of running
<code class="docutils literal notranslate"><span class="pre">checkproject</span></code> is below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ checkproject &lt;allocation&gt;

====================================
Reporting for project pXXXXX
------------------------------------
1 GB in 4623 files (0% of 1000 GB quota)
Allocation Type: Allocation I
Expiration Date: 2022-12-01
Status: ACTIVE
Compute and storage allocation - when status is ACTIVE, this allocation has compute node access and can submit jobs
------------------------------------
====================================
</pre></div>
</div>
<p>For more information on allocations and allocation management,
please see <a class="reference external" href="https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1486">Managing an Allocation on
Quest</a>.</p>
<p class="rubric" id="section-partitions">Quest Partitions/Queues</p>
<p>To specify the partition, please include a <code class="docutils literal notranslate"><span class="pre">-p/--partition</span></code>
option in your job submission script:</p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-p</span> <span class="pre">&lt;partition&gt;</span></code></p>
<p>Quest offers several partitions or queues where you can run your
job. Based on the duration of your job, number of cores, and type
of access to Quest, you should select the most appropriate
partition for your job. A partition must be specified when you
submit your job otherwise the scheduler will return the error,
<code class="docutils literal notranslate"><span class="pre">"sbatch:</span> <span class="pre">error:</span> <span class="pre">Batch</span> <span class="pre">job</span> <span class="pre">submission</span> <span class="pre">failed:</span> <span class="pre">No</span> <span class="pre">partition</span> <span class="pre">specified</span> <span class="pre">or</span> <span class="pre">system</span> <span class="pre">default</span> <span class="pre">partition"</span></code>.</p>
<p class="rubric" id="partition-definitions-general-access-p-and-e-accounts">Partition Definitions: General Access (“p” and “e”
accounts)</p>
<p>Standard compute node access.</p>
<div class="table-responsive docutils container">
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Partition</p></th>
<th class="head"><p>Minimum
Walltime</p></th>
<th class="head"><p>Maximum
Walltime</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>short</p></td>
<td><p>00:00:00</p></td>
<td><p>04:00:00</p></td>
<td><p>The short
partition is
for jobs that
will run in 4
hours or less.
The short
partition has
access to more
compute nodes
than the normal
or long
partitions.
This feature
plus the
shorter
duration
enables short
partition jobs
to be scheduled
faster.</p></td>
</tr>
<tr class="row-odd"><td><p>normal</p></td>
<td><p>04:00:00</p></td>
<td><p>48:00:00</p></td>
<td><p>The normal
partition is
for jobs that
will run in
between 4 hours
and 2 days. The
normal
partition has
access to more
compute nodes
than the long
partition, but
less than the
short
partition.</p></td>
</tr>
<tr class="row-even"><td><p>long</p></td>
<td><p>48:00:00</p></td>
<td><p>168:00:00</p></td>
<td><p>The long
partition is
for jobs that
will run in
between 2 days
and 7 days. The
long partition
has access to a
less compute
nodes than the
short or normal
partition.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Specialty compute node access</p>
<div class="table-responsive docutils container">
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Partition</p></th>
<th class="head"><p>Maximum Walltime</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>gengpu</p></td>
<td><p>48:00:00</p></td>
<td><p>This partition can be used only if
your job requires GPUs. In addition
to entering <em>gengpu</em> as your
partition in your submission
script, you must specify
<code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--gres=gpu:a100:X</span></code>,
where <code class="docutils literal notranslate"><span class="pre">X</span></code> is the number of GPUs
you want to request. Please see
<a class="reference external" href="https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1112">GPUs on
QUEST</a>
for more information about the
GPUs.</p></td>
</tr>
<tr class="row-odd"><td><p>genhimem</p></td>
<td><p>48:00:00</p></td>
<td><p>This partition can be used only if
your job requires more than 180 GB
memory per node. This partition has
access to 4 nodes, three 28-core
nodes with 500GB of schedulable
memory and one 52-core node with
1.5TB of schedulable memory.</p></td>
</tr>
</tbody>
</table>
</div>
<p class="rubric" id="partition-definitions-full-access-buy-ins-or-b-accounts">Partition Definitions: Full Access (buy-ins, or “b”
accounts)</p>
<div class="table-responsive docutils container">
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Partition</p></th>
<th class="head"><p>Maximum Walltime</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Allocation name (e.g.
“b1234”)
or
“buyin”</p></td>
<td><p>Allocation-specific</p></td>
<td><p>For those users part
of an allocation
which has
purchased<a class="reference external" href="https://it.northwestern.edu/secure/purchasing-resources.html">computing
access</a>
to Quest, the
resources available
and any limits on
jobs are governed by
the specific policies
of the full-access
allocation. In
addition to setting
the <code class="docutils literal notranslate"><span class="pre">-A/--account</span></code>
flag to your buy-in
allocation name, you
can set the partition
in one of two ways:</p>
<div class="line-block">
<div class="line">Example:</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-p</span> <span class="pre">bXXXXX</span></code>
| Example:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">#SBATCH-p</span> <span class="pre">buyin</span></code></p>
</div></blockquote>
<p>If your allocation
has specific
partition names, such
as <code class="docutils literal notranslate"><span class="pre">genomics</span></code>,
<code class="docutils literal notranslate"><span class="pre">ciera-std</span></code>,
<code class="docutils literal notranslate"><span class="pre">grail-std</span></code> etc.,
you should use those
partition names
instead of your
allocation name or
<code class="docutils literal notranslate"><span class="pre">buyin</span></code>.</p>
</td>
</tr>
</tbody>
</table>
</div>
<p class="rubric" id="notes">Notes</p>
<p>Additional specialized partitions exist for specific allocations.
You may be instructed to use a partition name that is not listed
above.</p>
<p>If you have a general access allocation and need to run jobs
longer than one week, <a class="reference external" href="mailto:quest-help%40northwestern.edu?subject=Quest%20Long-running%20job">contact Research
Computing</a>
for a consultation. Some special accommodations can be made for
jobs requiring the resources of up to a single node for a month or
less.</p>
<p class="rubric" id="section-walltime">Walltime/Length of the job</p>
<p>To specify the walltime for your job, please include a
<code class="docutils literal notranslate"><span class="pre">-t/--walltime</span></code> option in your job submission script:</p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-t</span> <span class="pre">&lt;timelimit&gt;</span></code></p>
<p>There are two important considerations when selecting the
walltime, the <a class="reference external" href="#section-partitions">partition</a> that you chose
and how long your job is expected to run. Although the partition
that you choose will control the maximum wall time that can be
selected, we do <em>not</em> recommend to simply select the maximum
allowable wall time for that partition unless it is truly needed.
There are two problematic ways to set wall time that will create
undesired outcomes:</p>
<ul class="simple">
<li><p>If your walltime is much longer than your job needs to run,
then your job will take longer to start running.</p></li>
<li><p>If your walltime is shorter than your job needs to run, then
your job will fail as there is no way to extend the walltime of
a running job on Quest.</p></li>
</ul>
<p>This is why we recommend submitting a single, representative job
and seeing how long it takes to run before selecting a walltime
and submitting a large number of jobs. Please note that Slurm
considers <em>only the</em><strong>actual</strong><em>duration of your job (not the
defined walltime) when calculating your resource utilization.</em></p>
<p class="rubric" id="section-number-of-nodes">Number of Nodes</p>
<p>To specify the number of nodes, please include the <code class="docutils literal notranslate"><span class="pre">-N/--nodes</span></code>
option in your job submission script:</p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--nodes=&lt;number_of_nodes&gt;</span></code></p>
<p>Although the number of nodes is an optional setting, we strongly
recommend always setting this value. Specifically, we recommend
setting this value to</p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--nodes=1</span></code></p>
<p>as the vast majority of software can only run on a single computer
and cannot run across multiple computers. When you forget to set
this value, but you do set the <a class="reference external" href="#number-of-cores">Number of
Cores</a>, this can cause Slurm to match you
with a set of computing resources which your application will be
unable to use, but will still be penalized in your fair share for
using. If you know that your application uses Message Passing
Interface (MPI) to parallelize, then setting this value to
something <code class="docutils literal notranslate"><span class="pre">&gt;1</span></code> could make sense.</p>
<p class="rubric" id="section-number-of-cores">Number of Cores</p>
<p>There are two methods for specifying the number of cores, the
<code class="docutils literal notranslate"><span class="pre">-n/--ntasks</span></code> option which indicates how many total cores you
would like:</p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--ntasks=&lt;number_of_cores&gt;</span></code></p>
<p>or the <code class="docutils literal notranslate"><span class="pre">--ntasks-per-node=</span></code> option whichindicates how many cores
you would like <em>per node</em> and should always be used with the
<a class="reference external" href="#number-of-nodes">Number of Nodes</a> option:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>#SBATCH --nodes=&lt;number_of_nodes&gt;
#SBATCH --ntasks-per-node=&lt;number_of_cores_per_node&gt;
</pre></div>
</div>
<p>Although the number of cores is an optional setting whose default
is 1, we strongly recommend always setting this value.
Specifically, we recommend setting this value (to start) to</p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--ntasks=1</span></code></p>
<p>The only situation in which <code class="docutils literal notranslate"><span class="pre">-n/--ntasks</span></code> should be greater than
1 is if the application you are using has the capability to be
parallelized. Many applications do <strong>not</strong> have this capability
and therefore it is best to start of setting this value to 1. If
your application is capable of parallelization, you will next want
to determine what type of parallelization it uses in order to set
this value correctly. For instance, if your application utilizes
shared memory parallelization (OpenMP, R’s doParallel, Python’s
multiprocessing, MATLAB local parpool, etc) then you can consider
setting this value to be greater than 1. However, shared memory
parallelization can only utilize CPUs <em>within a single computer</em>
and CPUs allocated <em>across</em> computers will go unused. Therefore,
if your code is parallelized in this manner, you must also specify</p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--nodes=1</span></code></p>
<p>If you know that your application uses Message Passing Interface
(MPI) to parallelize, then it can utilize CPUs allocated<em>across</em>
computers and therefore setting <code class="docutils literal notranslate"><span class="pre">-n/--ntasks</span></code> without also
setting <code class="docutils literal notranslate"><span class="pre">-N/--nodes</span></code> would make sense.</p>
<p>A final consideration when selecting the number of CPUs is how
many CPUs are available on each of the different
generations/families of compute nodes that make up Quest. Below is
a table which summarizes the relevant information.</p>
<div class="table-responsive docutils container">
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Node Family
Name</p></th>
<th class="head"><p>Number of CPUs</p></th>
<th class="head"><p>Amount of
*
<em>Schedulable*</em>
Memory/RAM</p></th>
<th class="head"><p>Partitions
with these
Nodes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>quest8
(general
access)</p></td>
<td><p>28</p></td>
<td><p>84GB</p></td>
<td><p>short/nor
mal/long/buyin</p></td>
</tr>
<tr class="row-odd"><td><p>quest8 (buyin)</p></td>
<td><p>28</p></td>
<td><p>180GB</p></td>
<td><p>buyin/short</p></td>
</tr>
<tr class="row-even"><td><p>quest9</p></td>
<td><p>40</p></td>
<td><p>180GB</p></td>
<td><p>buyin/short</p></td>
</tr>
<tr class="row-odd"><td><p>quest10</p></td>
<td><p>52</p></td>
<td><p>180GB</p></td>
<td><p>short/nor
mal/long/buyin</p></td>
</tr>
<tr class="row-even"><td><p>quest11</p></td>
<td><p>64</p></td>
<td><p>243GB</p></td>
<td><p>short/buyin</p></td>
</tr>
</tbody>
</table>
</div>
<p>To drive home this point, imagine you made the following request:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks-per-node=30</span>
<span class="c1">#SBATCH --partition=short</span>
</pre></div>
</div>
<p>This request would eliminate Slurm’s ability to match you with any
of the computers from generation quest8 and would increase the
amount of time it will take to schedule your job as only one type
of compute node is able to match your request.</p>
<p class="rubric" id="section-required-memory">Required memory</p>
<p>There are two methods for specifying how much memory/RAM you need,
the <code class="docutils literal notranslate"><span class="pre">--mem</span></code> option which indicates how much memory you
want<em>per node</em>.</p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--mem=&lt;memory</span> <span class="pre">per</span> <span class="pre">node&gt;G</span></code></p>
<p>or the <code class="docutils literal notranslate"><span class="pre">--mem-per-cpu</span></code> option which indicates how much
memory/RAM you need <em>per CPU.</em></p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--mem-per-cpu=&lt;memory</span> <span class="pre">per</span> <span class="pre">cpu&gt;G</span></code></p>
<p>If your job submission script does not specify how much memory
your job requires, then the default setting is 3.25 GB of memory
per core.</p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--mem-per-cpu=3256M</span></code></p>
<p>Therefore, you submitted a job to run on 10 cores and did not
specify your memory request in your job submission script, Slurm
will allocate 32.5 GB in total.</p>
<p>The memory that is allocated to your job via this setting creates
a <strong>hard upper limit</strong> and your application cannot access memory
beyond what Slurm reserves for them; if your job tries to access
more memory than has been reserved, it will be terminated.</p>
<p>There is a special setting to request the entire memory of the
computer.</p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--mem=0</span></code></p>
<p>How much memory this ends up being will depend on what
generation/family of computer Slurm matches you to. The following
is a table which summarizes the relevant information.</p>
<div class="table-responsive docutils container">
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Node Family
Name</p></th>
<th class="head"><p>Number of CPUs</p></th>
<th class="head"><p>Amount of
*
<em>Schedulable*</em>
Memory/RAM</p></th>
<th class="head"><p>Partitions
with these
Nodes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>quest8
(general
access)</p></td>
<td><p>28</p></td>
<td><p>84GB</p></td>
<td><p>short/nor
mal/long/buyin</p></td>
</tr>
<tr class="row-odd"><td><p>quest8 (buyin)</p></td>
<td><p>28</p></td>
<td><p>180GB</p></td>
<td><p>buyin/short</p></td>
</tr>
<tr class="row-even"><td><p>quest9</p></td>
<td><p>40</p></td>
<td><p>180GB</p></td>
<td><p>buyin/short</p></td>
</tr>
<tr class="row-odd"><td><p>quest10</p></td>
<td><p>52</p></td>
<td><p>180GB</p></td>
<td><p>short/nor
mal/long/buyin</p></td>
</tr>
<tr class="row-even"><td><p>quest11</p></td>
<td><p>64</p></td>
<td><p>243GB</p></td>
<td><p>short/buyin</p></td>
</tr>
</tbody>
</table>
</div>
<p>A final consideration when selecting the amount of memory/RAM is
the available memory/RAM on each of the different
generations/families of compute nodes that make up Quest. To drive
home this point, imagine you made the following request:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --mem=130G</span>
<span class="c1">#SBATCH --partition=short</span>
</pre></div>
</div>
<p>This request would eliminate Slurm’s ability to match you with any
of the computers from generation quest8 and would increase the
amount of time it will take to schedule your job as you will have
reduced the pool of available compute nodes.</p>
<p class="rubric" id="how-can-i-tell-if-my-job-needs-more-memory-to-run-successfully">How can I tell if my job needs more memory to run
successfully?</p>
<p>Use the <code class="docutils literal notranslate"><span class="pre">sacct</span> <span class="pre">-X</span></code> command to see information about your recent
jobs, for example:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>$ sacct -X
           JobID    JobName  Partition    Account  AllocCPUS      State ExitCode
    ------------ ---------- ---------- ---------- ---------- ---------- --------
    1273539      lammps-te+      short     p1234          40  COMPLETED      0:0
    1273543      vasp-open+      short     p1234          40 OUT_OF_ME+    0:125
</pre></div>
</div>
<div class="line-block">
<div class="line"><br/></div>
<div class="line">The “State” field is the status of your job when it finished.
Jobs with a “COMPLETED” state have run without system errors.
Jobs with an “OUT_OF_ME+” state have run out of memory and
failed. “OUT_OF_ME+” jobs need to request more memory in their
job submission scripts to complete successfully.</div>
<div class="line">If the job you’re investigating is not recent enough to be
listed by <code class="docutils literal notranslate"><span class="pre">sacct</span> <span class="pre">-X</span></code>, add date fields to the command to see
jobs between specific start and end dates. For example, to see
all jobs between September 15, 2019 and September 16, 2019:</div>
</div>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>$ sacct -X --starttime=091519 --endtime=091619
</pre></div>
</div>
<div class="line-block">
<div class="line"><br/></div>
<div class="line">Specify the date using MMDDYY. More information on sacct is
available <a class="reference external" href="https://slurm.schedmd.com/sacct.html">here</a>.</div>
</div>
<p class="rubric" id="my-job-ran-out-of-memory-and-failed-now-what">My job ran out of memory and failed, now what?</p>
<p>First, determine how much memory your job needs following the
steps outlined below. Once you know how much memory your job
needs, edit your job submission script to reserve that amount of
memory + 10% for your job.</p>
<p>To find your job’s memory utilization on a compute node:</p>
<ol class="arabic simple">
<li><p>create a test job by editing your job’s submission script to
reserve all of the memory of the node it runs on</p></li>
<li><p>run your test job</p></li>
<li><p>confirm your test job has completed successfully</p></li>
<li><p>use <code class="docutils literal notranslate"><span class="pre">seff</span></code> to see how much memory your job actually used.</p></li>
</ol>
<p><em>Create a test job</em></p>
<p>To profile your job’s memory usage, create a test job by modifying
your job’s submission script to include the lines:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>#SBATCH --mem=0
#SBATCH --nodes=1
</pre></div>
</div>
<div class="line-block">
<div class="line">Setting <code class="docutils literal notranslate"><span class="pre">--mem=0</span></code> reserves all of the memory on the node for
your job; if you already have a <code class="docutils literal notranslate"><span class="pre">--mem=</span></code> directive in your job
submission script, comment it out. Now your job will not run out
of memory unless your job needs more memory than is available on
the node.</div>
<div class="line">Setting <code class="docutils literal notranslate"><span class="pre">--nodes=1</span></code> reserves a single node for your job. For
jobs that run on multiple nodes such as MPI-based programs,
request the number of nodes that your job utilizes. Be sure to
specify a value for <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--nodes=</span></code> or the cores your job
submission script reserves could end up on as many nodes as
cores requested. Be aware that by setting <code class="docutils literal notranslate"><span class="pre">--mem=0</span></code>, you will
be reserving all the memory on each of those nodes that your
cores are reserved on.</div>
</div>
<div class="line-block">
<div class="line">2) <a href="#id1"><span class="problematic" id="id2">*</span></a>Run your test job
*</div>
<div class="line">Submit your test job to the cluster with the <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> command.
For interactive jobs, use <code class="docutils literal notranslate"><span class="pre">srun</span></code> or <code class="docutils literal notranslate"><span class="pre">salloc</span></code>.</div>
<div class="line">3) <em>Did your test job complete successfully?</em></div>
<div class="line">When your job has stopped running, use the <code class="docutils literal notranslate"><span class="pre">sacct</span> <span class="pre">-X</span></code> command
to confirm your job finished with state “COMPLETED”. If your
test job finishes with an “OUT_OF_ME+” state, confirm that you
are submitting the modified job submission script that requests
all of the memory on the node. If the “OUT_OF_ME+” errors
persist, your job may require more memory than is available on
the compute node it ran on. In this case, please email
<a class="reference external" href="mailto:quest-help%40northwestern.edu">quest-help<span>@</span>northwestern<span>.</span>edu</a> for assistance.</div>
<div class="line">4) <em>How much memory did your job actually use?</em></div>
<div class="line">To see how much memory it used run the command:
<code class="docutils literal notranslate"><span class="pre">seff</span> <span class="pre">&lt;test_job_id_number&gt;</span></code>. This returns output similar to:</div>
</div>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>Job ID: 767731
    Cluster: quest
    User/Group: abc123/abc123
    State: COMPLETED (exit code 0)
    Cores: 1
    CPU Utilized: 00:10:00
    CPU Efficiency: 100.00% of 00:10:00 core-walltime
    Job Wall-clock time: 00:10:00
    Memory Utilized: 60.00 GB
    Memory Efficiency: 50.00% of 120.00 GB
</pre></div>
</div>
<p>Check the job State reported in the 4th line. If it is “COMPLETED
(exit code 0)”, look at the last two lines. “Memory Utilized” is
the amount of memory your job used, in this case 60Gb.</p>
<p>If the job State is FAILED or CANCELLED, the Memory Efficiency
percentage reported by <code class="docutils literal notranslate"><span class="pre">seff</span></code>will be extremely inaccurate. The
<code class="docutils literal notranslate"><span class="pre">seff</span></code> command only works on jobs that have COMPLETED
successfully.</p>
<p class="rubric" id="how-much-memory-should-i-reserve-in-my-job-script">How much memory should I reserve in my job script?</p>
<p>It’s a good idea to reserve slightly more memory than your job
utilized since the same job may require slightly different amounts
of memory depending on variations in data it processes in each run
of the job. To correctly reserve memory for this job, edit your
test job submission script to modify the <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--mem=</span></code>
directive to reserve 10% more than 60Gb in the job submission
script:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>#SBATCH --mem=66G
</pre></div>
</div>
<p>For jobs that use MPI, remove the <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--mem=</span></code> directive
from your job submission script. Now specify the amount of memory
you’d like to reserve per core instead. For example, if your job
uses 100Gb of memory total and runs on 10 cores, reserve 10Gb plus
a safety factor per cpu:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>#SBATCH --mem-per-cpu=11G
</pre></div>
</div>
<p>If it doesn’t matter how many nodes your cores are distributed on,
you may remove the <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--nodes=</span></code> directive as well.</p>
<p>Be careful not to reserve significant amounts of memory beyond
what your job requires as your job’s wait time will increase.
Reserving excessive memory also wastes shared resources that could
be used by other researchers.</p>
<p class="rubric" id="section-output-error">Standard Output/Error</p>
<p>To specify a file into which <em>both</em> the standard output <em>and</em>
standard error from your job will be written, please include
<em>only</em> the <code class="docutils literal notranslate"><span class="pre">-o/--output</span></code> option in your job submission script:</p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--output=&lt;name</span> <span class="pre">of</span> <span class="pre">file&gt;.out</span></code></p>
<p>This will cause a file to be created in the submission directory
with the name you defined. You may also specify filename which
includes the absolute or full path to the file <strong>but you cannot
just include a path to a directory</strong>. Please make sure that all
directories in the file path name exist on Quest.</p>
<p>To separate out the standard output and standard error into two
separate files, please include <em>both</em> the <code class="docutils literal notranslate"><span class="pre">-o/--output</span></code> option
<em>and</em> the <code class="docutils literal notranslate"><span class="pre">-e/--error</span></code> option in your job submission script:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>#SBATCH --output=&lt;name of file&gt;.out
#SBATCH --error=&lt;name of file&gt;.err
</pre></div>
</div>
<p>If you do not include either option, the default setting will be
to write both the standard output and standard error from your job
in a file called</p>
<p><code class="docutils literal notranslate"><span class="pre">slurm-&lt;slurm</span> <span class="pre">jobid&gt;.out</span></code>
where <code class="docutils literal notranslate"><span class="pre">&lt;slurm</span> <span class="pre">jobid&gt;</span></code> is the ID given to your job by SLURM. You
can replicate this default naming scheme yourself by providing the
following option:</p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--output=slurm-%j.out</span></code></p>
<p>In addition to <code class="docutils literal notranslate"><span class="pre">%j</span></code> which will add the job id to the name of
your output file, there is also <code class="docutils literal notranslate"><span class="pre">%x</span></code> which will add the job name
to the name of your output file.</p>
<p class="rubric" id="section-job-name">Job Name</p>
<p>To specify a name for your job, please include a <code class="docutils literal notranslate"><span class="pre">-J/--job-name</span></code>
option in your job submission script:</p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--job-name=&lt;job</span> <span class="pre">name&gt;</span></code></p>
<p>The default is to set this value to the name of the submission
file, so we recommend that you set this to a memorable string
because it can be useful when trying to identify a specific job
among several running or completed jobs.</p>
<p class="rubric" id="section-email">Sending e-mail alerts about your job</p>
<p>To receive e-mails regarding the status of your Slurm jobs, please
include <em>both</em> the <code class="docutils literal notranslate"><span class="pre">--mail-type</span></code> option <em>and</em> the
<code class="docutils literal notranslate"><span class="pre">--mail-user</span></code> option in your job submission script:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>#SBATCH --mail-type=&lt;job state that triggers email&gt; ## BEGIN, END, FAIL or ALL
#SBATCH --mail-user=&lt;email address&gt;
</pre></div>
</div>
<p>If you do not include both of these options, then you will not
receive emails from Slurm. Also, you can include any combination
of BEGIN, END, FAIL as an argument for this option.</p>
<p class="rubric" id="section-constraints">Constraints</p>
<p>To specify an architecture constraint for your job, please include
a <code class="docutils literal notranslate"><span class="pre">-C/--constraint</span></code> option in your job submission script:</p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--constraint=&lt;name</span> <span class="pre">of</span> <span class="pre">compute</span> <span class="pre">node</span> <span class="pre">architecture&gt;</span></code></p>
<p>Not all Quest compute nodes are the same. We currently have four
different generations or architectures of compute nodes which we
refer to as quest8, quest9 and quest10 and a summary table of
these architectures is provided below.</p>
<div class="table-responsive docutils container">
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Node Family
Name</p></th>
<th class="head"><p>Number of CPUs</p></th>
<th class="head"><p>Amount of
*
<em>Schedulable*</em>
Memory/RAM</p></th>
<th class="head"><p>Partitions
with these
Nodes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>quest8
(general
access)</p></td>
<td><p>28</p></td>
<td><p>84GB</p></td>
<td><p>short/nor
mal/long/buyin</p></td>
</tr>
<tr class="row-odd"><td><p>quest8 (buyin)</p></td>
<td><p>28</p></td>
<td><p>180GB</p></td>
<td><p>buyin/short</p></td>
</tr>
<tr class="row-even"><td><p>quest9</p></td>
<td><p>40</p></td>
<td><p>180GB</p></td>
<td><p>buyin/short</p></td>
</tr>
<tr class="row-odd"><td><p>quest10</p></td>
<td><p>52</p></td>
<td><p>180GB</p></td>
<td><p>short/nor
mal/long/buyin</p></td>
</tr>
<tr class="row-even"><td><p>quest11</p></td>
<td><p>64</p></td>
<td><p>243GB</p></td>
<td><p>short/buyin</p></td>
</tr>
</tbody>
</table>
</div>
<div class="line-block">
<div class="line">You can find detailed information on each of these architectures
<a class="reference external" href="https://it.northwestern.edu/departments/it-services-support/research/computing/quest/specs.html">here</a>.
If you need to restrict your job to a particular architecture,
you can do so through the constraint directive. For example,
<code class="docutils literal notranslate"><span class="pre">--constraint=quest10</span></code> will cause the scheduler to only match
you to compute nodes of the quest10 generation.</div>
<div class="line">Moreover, if you would like to match to any generation of
compute nodes, but would like all the compute nodes to be either
of generation quest8 or quest9 or quest10 or quest11 and not a
combination of generations, then you can set the following for
constraint.</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--constraint="[quest8|quest9|quest10|quest11]"</span></code></div>
<div class="line">This is recommended for jobs that are parallelized using MPI.</div>
</div>
<p class="rubric" id="section-all-slurm-options">All Slurm Configuration Options</p>
<div class="table-responsive docutils container">
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Option</p></th>
<th class="head"><p>Slurm (sbatch)</p></th>
<th class="head"><p>Default/Required</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Account</p></td>
<td><p>–account=&lt;account&gt;
-A &lt;account&gt;</p></td>
<td><p>Required: Not set by
default</p></td>
</tr>
<tr class="row-odd"><td><p>Partition/queue</p></td>
<td><p>–
partition=&lt;partition&gt;</p></td>
<td><p>Required: Not set by
default</p></td>
</tr>
<tr class="row-even"><td><p>Wall time limit</p></td>
<td><p>–time=&lt;hh:mm:ss&gt;
-t&lt;hh:mm:ss&gt;</p></td>
<td><p>Required: Not set by
default</p></td>
</tr>
<tr class="row-odd"><td><p>Job name</p></td>
<td><p>–job-name=&lt;name&gt;
-J &lt;name&gt;</p></td>
<td><p>Optional: Default is
to set this value to
the name of the
submission file.</p></td>
</tr>
<tr class="row-even"><td><p>Node count</p></td>
<td><p>–nodes=&lt;count&gt;
-N &lt;count&gt;</p></td>
<td><p>Optional: Not set by
default</p></td>
</tr>
<tr class="row-odd"><td><p>Core count</p></td>
<td><p>-n &lt;count&gt;</p>
<p>–ntasks=&lt;count&gt;</p>
</td>
<td><p>Optional: Default is
1</p></td>
</tr>
<tr class="row-even"><td><p>Process count per
node</p></td>
<td><p>–nt
asks-per-node=&lt;count&gt;</p></td>
<td><p>Optional: Not set by
default</p></td>
</tr>
<tr class="row-odd"><td><p>Core count (per
process)</p></td>
<td><p>–
cpus-per-task=&lt;cores&gt;</p></td>
<td><p>Optional: Default is
1</p></td>
</tr>
<tr class="row-even"><td><p>Memory per node</p></td>
<td><p>–mem=&lt;memory&gt;
(default unit MB)</p></td>
<td><p>Optional: Not set by
default</p></td>
</tr>
<tr class="row-odd"><td><p>Memory per processor</p></td>
<td><p>–mem-per-cpu=&lt;memory&gt;</p></td>
<td><p>Optional: Default is
3.25GB</p></td>
</tr>
<tr class="row-even"><td><p>Request GPUs</p></td>
<td><p>–gr
es=gpu:&lt;type&gt;:&lt;count&gt;</p></td>
<td><p>Optional: Not set by
default</p></td>
</tr>
<tr class="row-odd"><td><p>Instead of specifying
how many nodes you
want,
you could request a
specific set of
compute nodes.
This cannot be used
in combination with
the <code class="docutils literal notranslate"><span class="pre">--nodes=</span></code>
setting.</p></td>
<td><p>-w,
–nodelis
t=&lt;node&gt;[,node2[,…]]&gt;
-F, –nodefile=&lt;node
file&gt;</p></td>
<td><p>Optional: Not set by
default</p></td>
</tr>
<tr class="row-even"><td><p>Job array</p></td>
<td><p>-a &lt;array indices&gt;</p></td>
<td><p>Optional: Not set by
default</p></td>
</tr>
<tr class="row-odd"><td><p>Standard output file</p></td>
<td><p>–output=&lt;file path&gt;
(path must exist)</p></td>
<td><p>Optional: Not set by
default</p></td>
</tr>
<tr class="row-even"><td><p>Standard error file</p></td>
<td><p>–error=&lt;file path&gt;
(path must exist)</p></td>
<td><p>Optional: Not set by
default</p></td>
</tr>
<tr class="row-odd"><td><p>Combine stdout/stderr
to stdout</p></td>
<td><p>–output=&lt;combined out
and err file path&gt;</p></td>
<td><p>Optional: Set to
<code class="docutils literal notranslate"><span class="pre">slurm-&lt;jobid&gt;.out</span></code>
by default.</p></td>
</tr>
<tr class="row-even"><td><p>Architecture
constraint</p></td>
<td><p>–cons
traint=&lt;architecture&gt;
-C &lt;architecture&gt;</p></td>
<td><p>Optional: Not set by
default</p></td>
</tr>
<tr class="row-odd"><td><p>Copy environment</p></td>
<td><p>–export=ALL (default)
–export=NONE ## to
not export
environment</p></td>
<td><p>Optional: Default is
to export ALL
environmental
settings from the
submission
environment to the
runtime environment.</p></td>
</tr>
<tr class="row-even"><td><p>Copy environment
variable</p></td>
<td><p>–export
=&lt;variable[=value][,v
ariable2=value2[,…]]&gt;</p></td>
<td><p>Optional: Not set by
default</p></td>
</tr>
<tr class="row-odd"><td><p>Job dependency</p></td>
<td><p>–dependency
=after:jobID[:jobID…]
–dependency=a
fterok:jobID[:jobID…]
–dependency=afte
rnotok:jobID[:jobID…]
–dependency=af
terany:jobID[:jobID…]</p></td>
<td><p>Optional: Not set by
default</p></td>
</tr>
<tr class="row-even"><td><p>Request event
notification</p></td>
<td><p>–mail-type=&lt;events&gt;
Note: multiple
mail-type requests
may be specified in a
comma separated list:
–mai
l-type=BEGIN,END,FAIL</p></td>
<td><p>Optional: Not set by
default</p></td>
</tr>
<tr class="row-odd"><td><p>Email address</p></td>
<td><p>–mail-user=&lt;email
address&gt;</p></td>
<td><p>Optional: Not set by
default</p></td>
</tr>
<tr class="row-even"><td><p>Defer job until the
specified time</p></td>
<td><p>–begin=&lt;date/time&gt;</p></td>
<td><p>Optional: Not set by
default</p></td>
</tr>
<tr class="row-odd"><td><p>Node exclusive job</p></td>
<td><p>–exclusive</p></td>
<td><p>Optional: Not set by
default</p></td>
</tr>
</tbody>
</table>
</div>
<p class="rubric" id="section-slurm-environmental-variables">Environmental Variables Set by Slurm</p>
<p>The variables in the following table are set by Slurm and they are
accessbile in the environment of your job after the job has
started running</p>
<div class="table-responsive docutils container">
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Info</p></th>
<th class="head"><p>Slurm</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Job name</p></td>
<td><p>$SLURM_JOB_NAME</p></td>
</tr>
<tr class="row-odd"><td><p>Job ID</p></td>
<td><p>$SLURM_JOB_ID</p></td>
</tr>
<tr class="row-even"><td><p>Submit directory</p></td>
<td><p>$SLURM_SUBMIT_DIR</p></td>
</tr>
<tr class="row-odd"><td><p>Node list</p></td>
<td><div class="line-block">
<div class="line">$SLURM_JOB_NODELIST</div>
<div class="line">$SLURM_NODELIST</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>Job array index</p></td>
<td><p>$SLURM_ARRAY_TASK_ID</p></td>
</tr>
<tr class="row-odd"><td><p>Queue name</p></td>
<td><p>$SLURM_JOB_PARTITION</p></td>
</tr>
<tr class="row-even"><td><p>Number of nodes allocated</p></td>
<td><p>$SLURM_JOB_NUM_NODES
$SLURM_NNODES</p></td>
</tr>
<tr class="row-odd"><td><p>Number of processes</p></td>
<td><p>$SLURM_NTASKS</p></td>
</tr>
<tr class="row-even"><td><p>Number of processes per node</p></td>
<td><p>$SLURM_TASKS_PER_NODE</p></td>
</tr>
<tr class="row-odd"><td><p>Requested tasks per node</p></td>
<td><p>$SLURM_NTASKS_PER_NODE</p></td>
</tr>
<tr class="row-even"><td><p>Requested CPUs per task</p></td>
<td><p>$SLURM_CPUS_PER_TASK</p></td>
</tr>
<tr class="row-odd"><td><p>Scheduling priority</p></td>
<td><p>$SLURM_PRIO_PROCESS</p></td>
</tr>
<tr class="row-even"><td><p>Job user</p></td>
<td><p>$SLURM_JOB_USER</p></td>
</tr>
<tr class="row-odd"><td><p>Log In Node from which this job
was submitted.</p></td>
<td><p>$SLURM_SUBMIT_HOST</p></td>
</tr>
</tbody>
</table>
</div>
<p class="rubric" id="all-slurm-commands-and-submission-options">SLURM Commands and Job Management</p>
<p>In this section, we discuss how to manage batch jobs after they
have been submitted on Quest. This includes how to monitor jobs
currently pending or running, how to cancel jobs, and how to check
on the status of past jobs.</p>
<p class="rubric" id="section-common-slurm-commands">Table of Common Slurm Commands</p>
<div class="table-responsive docutils container">
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Option</p></th>
<th class="head"><p>Slurm (sbatch)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Submit a job</p></td>
<td><p>sbatch &lt;job script&gt;</p></td>
</tr>
<tr class="row-odd"><td><p>Delete a job</p></td>
<td><p>scancel &lt;job ID&gt;</p></td>
</tr>
<tr class="row-even"><td><p>Job status (by job)</p></td>
<td><p>squeue -j &lt;job ID&gt;</p></td>
</tr>
<tr class="row-odd"><td><p>Job status (by user)</p></td>
<td><p>squeue -u &lt;netID&gt;</p></td>
</tr>
<tr class="row-even"><td><p>Job status (detailed)</p></td>
<td><p>scontrol show job -dd &lt;job ID&gt;
checkjob &lt;job ID&gt;</p></td>
</tr>
<tr class="row-odd"><td><p>Show expected start time</p></td>
<td><p>squeue -j &lt;job ID&gt; –start</p></td>
</tr>
<tr class="row-even"><td><p>Queue list / info</p></td>
<td><p>scontrol show partition [queue]</p></td>
</tr>
<tr class="row-odd"><td><p>Hold a job</p></td>
<td><p>scontrol hold &lt;job ID&gt;</p></td>
</tr>
<tr class="row-even"><td><p>Release a job</p></td>
<td><p>scontrol release &lt;job ID&gt;</p></td>
</tr>
<tr class="row-odd"><td><p>Start an interactive job</p></td>
<td><p>salloc &lt;args&gt;
srun –pty &lt;args&gt;</p></td>
</tr>
<tr class="row-even"><td><p>X forwarding</p></td>
<td><p>srun –pty &lt;args&gt; –x11</p></td>
</tr>
<tr class="row-odd"><td><p>Monitor or review a job’s
resource usage</p></td>
<td><p>sacct -j &lt;job_num&gt; –format
JobID,jobname,NTask
s,nodelist,CPUTime,ReqMem,Elapsed
(see sacct for all format
options)</p></td>
</tr>
<tr class="row-even"><td><p>View job batch script</p></td>
<td><p>sacct -j &lt;job_num&gt;
-B/–batch-script</p></td>
</tr>
</tbody>
</table>
</div>
<p class="rubric" id="squeue">The <code class="docutils literal notranslate"><span class="pre">squeue</span></code> Command</p>
<p>The <code class="docutils literal notranslate"><span class="pre">squeue</span></code> command can be used display information about your
current jobs on Quest.</p>
<div class="table-responsive docutils container">
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Command</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>squeue -u &lt;NetID&gt;</p></td>
<td><p>Show only jobs belonging to user specified</p></td>
</tr>
<tr class="row-odd"><td><p>squeue -A &lt;AllocationID&gt;</p></td>
<td><p>Show only jobs belonging to account specified</p></td>
</tr>
<tr class="row-even"><td><p>squeue -j &lt;JobID&gt;</p></td>
<td><p>Display the status of the specified job</p></td>
</tr>
<tr class="row-odd"><td><p>squeue -t R</p></td>
<td><p>Show running jobs</p></td>
</tr>
<tr class="row-even"><td><p>squeue -t PD</p></td>
<td><p>Show pending jobs</p></td>
</tr>
<tr class="row-odd"><td><p>squeue –help</p></td>
<td><p>See documentation and additional options</p></td>
</tr>
</tbody>
</table>
</div>
<p class="rubric" id="sacct">The <code class="docutils literal notranslate"><span class="pre">sacct</span></code> Command</p>
<p>The <code class="docutils literal notranslate"><span class="pre">sacct</span></code> command can be used display information about your
past and current jobs on Quest.</p>
<p><em>Starting with Slurm 22.05, there is a new feature of ``sacct``
which allows users to query the job submission script for a given
Slurm job for up to 1 year.</em> The syntax is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sacct</span> <span class="o">-</span><span class="n">j</span> <span class="o">&lt;</span><span class="n">job_num</span><span class="o">&gt;</span> <span class="o">-</span><span class="n">B</span>
</pre></div>
</div>
<p>Please note that the <code class="docutils literal notranslate"><span class="pre">-j</span></code> option is required to query the
submission script. Please find an example of this use of <code class="docutils literal notranslate"><span class="pre">sacct</span></code>
below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ sacct -j 1454461 -B
Batch Script for 1454461
--------------------------------------------------------------------------------
#!/bin/bash
#SBATCH -A a9009
#SBATCH -p all
#SBATCH --gres=gpu:p100:1
#SBATCH --output=anaconda_pytorch.txt
#SBATCH --job=anaconda_pytorch
#SBATCH -N 1
#SBATCH -n 1
#SBATCH -t 0:10:00
#SBATCH --mem=20G
#SBATCH --nodelist=qgpu6038

module purge all
module load python-miniconda3/4.12.0
source activate /projects/intro/envs/pytorch-1.10-py38
python training_pytorch.py
source deactivate
source activate /projects/intro/envs/pytorch-1.11-py38
python training_pytorch.py
</pre></div>
</div>
<p>Additionally, <code class="docutils literal notranslate"><span class="pre">sacct</span></code> can be used to provide specific
information about your Slurm jobs. Below, we demonstrate the
default/basic behavior of <code class="docutils literal notranslate"><span class="pre">sacct</span></code>. By default, <code class="docutils literal notranslate"><span class="pre">sacct</span></code> will
only display information about jobs from today and only a subset
of information: the jobid, jobname, partition, account, AllocCPUS,
state, and exitcode.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ sacct -X -u &lt;netid&gt;
JobID      JobName Partition  Account AllocCPUS   State ExitCode
------------ ---------- ---------- ---------- ---------- ---------- --------
1453894      bash    all   a9009     1 COMPLETED   0:0
1454434   sample_job    all   a9009     52   FAILED   6:0
</pre></div>
</div>
<p>The standard output of <code class="docutils literal notranslate"><span class="pre">sacct</span></code> may not provide the information
we want. To remedy this, we can use the <code class="docutils literal notranslate"><span class="pre">--format</span></code> flag to
choose what we want in our output. The format flag is handled by a
list of comma separated variables which specify output data:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ sacct --user=&lt;netid&gt; --format=var_1,var_2, ... ,var_N
</pre></div>
</div>
<p>A chart of some variables is provided below:</p>
<div class="table-responsive docutils container">
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Variable</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>account</p></td>
<td><p>Account the job ran under.</p></td>
</tr>
<tr class="row-odd"><td><p>cputime</p></td>
<td><p>Formatted (Elapsed time * CPU) count used by a job or step.</p></td>
</tr>
<tr class="row-even"><td><p>elapsed</p></td>
<td><p>Jobs elapsed time formated as DD-HH:MM:SS.</p></td>
</tr>
<tr class="row-odd"><td><p>exitcode</p></td>
<td><p>The exit code returned by the job script or salloc.</p></td>
</tr>
<tr class="row-even"><td><p>jobid</p></td>
<td><p>The id of the Job.</p></td>
</tr>
<tr class="row-odd"><td><p>jobname</p></td>
<td><p>The name of the Job.</p></td>
</tr>
<tr class="row-even"><td><p>ncpus</p></td>
<td><p>Amount of allocated CPUs.</p></td>
</tr>
<tr class="row-odd"><td><p>nnodes</p></td>
<td><p>The number of nodes used in a job.</p></td>
</tr>
<tr class="row-even"><td><p>ntasks</p></td>
<td><p>Number of tasks in a job.</p></td>
</tr>
<tr class="row-odd"><td><p>priority</p></td>
<td><p>Slurm priority.</p></td>
</tr>
<tr class="row-even"><td><p>qos</p></td>
<td><p>Quality of service.</p></td>
</tr>
<tr class="row-odd"><td><p>user</p></td>
<td><p>Username of the person who ran the job.</p></td>
</tr>
</tbody>
</table>
</div>
<p>In addition, only jobs from today will be displayed. To change
this, you can supply a start time and an end time via the
<code class="docutils literal notranslate"><span class="pre">--starttime</span></code> and <code class="docutils literal notranslate"><span class="pre">--endtime</span></code> flags, respectively. For
example, suppose you want to find information about jobs that were
run on September 12, 2022 and you want to show information
regarding the job name, the number of nodes used in the job, the
number of cpus, and the elapsed time. Your command would look like
this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ sacct -X --starttime=09/12/22 --format=jobname,nnodes,ncpus,elapsed
</pre></div>
</div>
<p>A full list of variables that specify data handled by <code class="docutils literal notranslate"><span class="pre">sacct</span></code>
can be found with the <code class="docutils literal notranslate"><span class="pre">--helpformat</span></code> flag or by <a class="reference external" href="https://slurm.schedmd.com/sacct.html">visiting the
slurm page on sacct</a>.</p>
<p class="rubric" id="seff">The <code class="docutils literal notranslate"><span class="pre">seff</span></code> Command</p>
<p>To see the maximum amount of memory used/needed by your job run
the command: <code class="docutils literal notranslate"><span class="pre">seff</span> <span class="pre">&lt;job_id&gt;</span></code>. This returns output similar to:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>Job ID: 767731
    Cluster: quest
    User/Group: abc123/abc123
    State: COMPLETED (exit code 0)
    Cores: 1
    CPU Utilized: 00:10:00
    CPU Efficiency: 100.00% of 00:10:00 core-walltime
    Job Wall-clock time: 00:10:00
    Memory Utilized: 60.00 GB
    Memory Efficiency: 50.00% of 120.00 GB
</pre></div>
</div>
<p>Check the job State reported in the 4th line. If it is “COMPLETED
(exit code 0)”, look at the last two lines. “Memory Utilized” is
the amount of memory your job used, in this case 60Gb.</p>
<p>If the job State is FAILED or CANCELLED, the Memory Efficiency
percentage reported by <code class="docutils literal notranslate"><span class="pre">seff</span></code>will be extremely inaccurate.
<em>The ``seff`` command only works on jobs that have COMPLETED
successfully.</em></p>
<p class="rubric" id="checkjob">The <code class="docutils literal notranslate"><span class="pre">checkjob</span></code> Command</p>
<p>The <code class="docutils literal notranslate"><span class="pre">checkjob</span></code> command displays detailed information about a
submitted job’s status and diagnostic information that can be
useful for troubleshooting submission issues. It can also be used
to obtain useful information about completed jobs such as the
allocated nodes, resources used, and exit codes.</p>
<p>Example usage:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>checkjob &lt;JobID&gt;
</pre></div>
</div>
<p>where you can get your &lt;JobID&gt; using the squeue commands above.</p>
<p class="rubric" id="example-for-a-successfully-running-job">Example for a Successfully Running Job</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>[abc123@quser21 ~]$ checkjob 548867
--------------------------------------------------------------------------------------------------------------------
JOB INFORMATION
--------------------------------------------------------------------------------------------------------------------
JobId=548867 JobName=high-throughput-cpu_000094
   UserId=abc123(123123) GroupId=abc123(123) MCS_label=N/A
   Priority=1315 Nice=0 Account=p12345 QOS=normal
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:13:13 TimeLimit=00:40:00 TimeMin=N/A
   SubmitTime=2019-01-22T12:51:42 EligibleTime=2019-01-22T12:51:43
   AccrueTime=2019-01-22T12:51:43
   StartTime=2019-01-22T15:52:20 EndTime=2019-01-22T16:32:20 Deadline=N/A
   PreemptTime=None SuspendTime=None SecsPreSuspend=0
   LastSchedEval=2019-01-22T15:52:20
   Partition=short AllocNode:Sid=quser21:15454
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=qnode[5056-5060]
   BatchHost=qnode5056
   NumNodes=5 NumCPUs=120 NumTasks=120 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   TRES=cpu=120,mem=360G,node=5,billing=780
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=1 MinMemoryCPU=3G MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=(null)
   WorkDir=/projects/p12345/high-throughput
   StdErr=/projects/p12345/high-throughput/lammps.error
   StdIn=/dev/null
   StdOut=/projects/p12345/high-throughput/lammps.output
   Power=
--------------------------------------------------------------------------------------------------------------------
JOB SCRIPT
--------------------------------------------------------------------------------------------------------------------
#!/bin/bash
#SBATCH --account=p12345
#SBATCH --partition=normal
#SBATCH --job-name=high-throughput-cpu
#SBATCH --ntasks=120
#SBATCH --mem-per-cpu=3G
#SBATCH --time=00:40:00
#SBATCH --error=lammps.error
#SBATCH --output=lammps.output

module purge
module load lammps/lammps-22Aug18

mpirun -n 120 lmp -in in.fcc
</pre></div>
</div>
<p>Note in the output above that:</p>
<ul class="simple">
<li><p>The JobState is listed as RUNNING.</p></li>
<li><p>The time passed after job start and the total walltime request
are given with RunTime and TimeLimit.</p></li>
<li><p>The node name(s) are listed after NodeList.</p></li>
<li><p>The paths to job’s working directory (WorkDir), standard error
(StdErr) and output (StdOut) files are given.</p></li>
<li><p>If a batch job script is used for submission, the script is
presented at the end.</p></li>
</ul>
<p class="rubric" id="scancel">Cancelling Jobs</p>
<p>You can cancel one or all of your jobs with <code class="docutils literal notranslate"><span class="pre">scancel</span></code>. Proceed
with caution, as this cannot be undone, and you will not be
prompted for confirmation after issuing the command.</p>
<div class="table-responsive docutils container">
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Command</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>scancel &lt;JobID&gt;</p></td>
<td><p>Cancel the job with given ID</p></td>
</tr>
<tr class="row-odd"><td><p>scancel -u &lt;NetID&gt;</p></td>
<td><p>Cancel all the jobs of the user</p></td>
</tr>
</tbody>
</table>
</div>
<p class="rubric" id="holding-releasing-or-modifying-jobs">Holding, Releasing, or Modifying Jobs</p>
<p>Users can place their jobs in a “JobHeldUser” state while
submitting the job or after the job has been queued. Running jobs
cannot be placed on hold.</p>
<div class="table-responsive docutils container">
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Command</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>#SBATCH -H</p></td>
<td><p>Place hold within job script</p></td>
</tr>
<tr class="row-odd"><td><p>sbatch -H &lt;jobscript&gt;</p></td>
<td><p>Place hold while submitting from command line</p></td>
</tr>
<tr class="row-even"><td><p>scontrol hold &lt;jobID&gt;</p></td>
<td><p>Place hold on a queued job from command line</p></td>
</tr>
</tbody>
</table>
</div>
<p>The job status will be shown in the output of monitoring commands
such as <code class="docutils literal notranslate"><span class="pre">squeue</span></code> or <code class="docutils literal notranslate"><span class="pre">checkjob</span></code>.</p>
<p>To release a job from user hold state:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>scontrol release &lt;JobID&gt;
</pre></div>
</div>
<p>The job control command (<code class="docutils literal notranslate"><span class="pre">scontrol</span></code>) can also be used for
changing the parameters of a submitted job <strong>before</strong> it starts
running. The following parameters can be modified safely:</p>
<ul class="simple">
<li><p>Job dependency (change to “none”)</p></li>
<li><p>Partition (queue)</p></li>
<li><p>Job name</p></li>
<li><p>Wall clock limit</p></li>
<li><p>Allocation</p></li>
</ul>
<p>The table below contains some useful examples of using
<code class="docutils literal notranslate"><span class="pre">scontrol</span></code> to change a job’s parameters.</p>
<div class="table-responsive docutils container">
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Command</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>scontrol update job=&lt;JobID&gt;
dependency=afterok:1000</p></td>
<td><p>Change job to depend successful
completion of the job 1000</p></td>
</tr>
<tr class="row-odd"><td><p>scontrol update job=&lt;JobID&gt;
partition=short</p></td>
<td><p>Change partition to short</p></td>
</tr>
<tr class="row-even"><td><p>scontrol update job=&lt;JobID&gt;
name=myjob</p></td>
<td><p>Change name to myjob</p></td>
</tr>
<tr class="row-odd"><td><p>scontrol update job=&lt;JobID&gt;
timelimit=2:00:00</p></td>
<td><p>Set job time limit to 2 hours</p></td>
</tr>
<tr class="row-even"><td><p>scontrol update job=&lt;JobID&gt;
account=p12345</p></td>
<td><p>Change the allocation to p12345</p></td>
</tr>
</tbody>
</table>
</div>
<p>For a complete listing of scontrol options, see the official
<a class="reference external" href="https://slurm.schedmd.com/scontrol.html">scontrol
documentation</a>.</p>
<p class="rubric" id="probing-priority">Probing Priority</p>
<p>Slurm implements a multi-factor priority scheme for ordering the
queue of jobs waiting to be run. <code class="docutils literal notranslate"><span class="pre">sprio</span></code> command is used to see
the contribution of different factors to a pending job’s
scheduling priority.</p>
<div class="table-responsive docutils container">
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Command</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>sprio</p></td>
<td><p>Show scheduling priority for all pending jobs
for the user</p></td>
</tr>
<tr class="row-odd"><td><p>sprio -j &lt;jobID&gt;</p></td>
<td><p>Show scheduling priority of the defined job</p></td>
</tr>
</tbody>
</table>
</div>
<p>For running jobs, you can see the starting priority using
<code class="docutils literal notranslate"><span class="pre">checkjob</span> <span class="pre">&lt;jobID&gt;</span></code> command.</p>
<p class="rubric" id="section-special-job-types">Special Types of Job Submissions</p>
<p>In this section, we provide details and examples of how to use
Slurm to run interactive jobs, job arrays, and jobs that depend on
each other.</p>
<p class="rubric" id="section-interactive-jobs">Interactive Job Examples</p>
<p class="rubric" id="section-section-interactive-jobs-non-gui">Submitting an Interactive Job (to run an application
<em>without</em> Graphical User Interface - GUI)</p>
<p>To launch an interactive job from the Quest log-in node in order
to run an application <em>without</em> a GUI, use either the
<a class="reference external" href="https://slurm.schedmd.com/srun.html">srun</a> or
<a class="reference external" href="https://slurm.schedmd.com/salloc.html">salloc</a> command. If you
use <code class="docutils literal notranslate"><span class="pre">srun</span></code> to run an interactive job, then SLURM will
automatically launch a terminal session on the compute node after
it schedules the job and you simply need to wait for this to
happen. <em>Due to the behavior of</em> <code class="docutils literal notranslate"><span class="pre">srun</span></code><em>, if you lose
connection to your interactive session, the interactive job will
terminate.</em></p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>[quser23 ~]$srun -N 1 -n 1 --account=&lt;account&gt; --mem=&lt;memory&gt;G --partition=&lt;partition&gt; --time=&lt;hh:mm:ss&gt; --pty bash -l
srun: job 3201233 queued and waiting for resources
srun: job 3201233 has been allocated resources
----------------------------------------
srun job start: Mon Mar 14 13:25:41 CDT 2022
Job ID: 3201233
Username: &lt;netid&gt;
Queue: &lt;partition&gt;
Account: &lt;account&gt;
----------------------------------------
The following variables are not
guaranteed to be the same in
prologue and the job run script
----------------------------------------
PATH (in prologue) : /hpc/usertools:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/lpp/mmfs/bin:/opt/ibutils/bin
WORKDIR is: /home/&lt;netid&gt;
----------------------------------------
[qnode0114 ~]$
</pre></div>
</div>
<p>If you use <code class="docutils literal notranslate"><span class="pre">salloc</span></code> instead, it will <em>not</em> automatically launch
a terminal session on the compute node. Instead, after it
schedules your job/request, it will tell you the name of the
compute node at which point you can run <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">qnodeXXXX</span></code> to
directly connect to the compute node. <em>Due to the behavior of</em>
<code class="docutils literal notranslate"><span class="pre">salloc</span></code><em>, if you lose connection to your interactive session,
the interactive job will</em><strong>not</strong><a href="#id3"><span class="problematic" id="id4">*</span></a>terminate.
*</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[quser21 ~]$ salloc -N 1 -n 1 --account=&lt;account&gt; --mem=&amp;ltmemory&gt;G --partition=&lt;partition&gt; --time=&lt;hh:mm:ss&gt;
salloc: Pending job allocation 276305
salloc: job 276305 queued and waiting for resources
salloc: job 276305 has been allocated resources
salloc: Granted job allocation 276305
salloc: Waiting for resource configuration
salloc: Nodes qnode8029 are ready for job
[quser21 ~]$ ssh qnode8029
Warning: Permanently added 'qnode8029,172.20.134.29' (ECDSA) to the list of known hosts.
[qnode8029 ~]$
</pre></div>
</div>
<p class="rubric" id="section-section-interactive-jobs-gui">Submitting an Interactive Job (to run an application
<em>with</em> Graphical User Interface)</p>
<p>To launch an interactive job from the Quest log-in node in order
to run an application <em>with</em> a GUI, first you need to connect to
Quest using an application with X11 forwarding support. We
recommend <a class="reference external" href="https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1511">using the FastX3
client</a>.
Once you have connected to Quest with X11 forwarding enabled, you
can then use either the
<a class="reference external" href="https://slurm.schedmd.com/srun.html">srun</a> or
<a class="reference external" href="https://slurm.schedmd.com/salloc.html">salloc</a> command. If you
use <code class="docutils literal notranslate"><span class="pre">srun</span></code> to run an interactive job, then SLURM will
automatically launch a terminal session on the compute node after
it schedules the job and you simply need to wait for this to
happen. <em>Due to the behavior of</em> <code class="docutils literal notranslate"><span class="pre">srun</span></code><em>, if you lose
connection to your interactive session, the interactive job will
terminate.</em></p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>[quser23 ~]$srun --x11 -N 1 -n 1 --account=&lt;account&gt; --mem=&lt;memory&gt;G --partition=&lt;partition&gt; --time=&lt;hh:mm:ss&gt; --pty bash -l
srun: job 3201233 queued and waiting for resources
srun: job 3201233 has been allocated resources
----------------------------------------
srun job start: Mon Mar 14 13:25:41 CDT 2022
Job ID: 3201233
Username: &lt;netid&gt;
Queue: &lt;partition&gt;
Account: &lt;account&gt;
----------------------------------------
The following variables are not
guaranteed to be the same in
prologue and the job run script
----------------------------------------
PATH (in prologue) : /hpc/usertools:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/lpp/mmfs/bin:/opt/ibutils/bin
WORKDIR is: /home/&lt;netid&gt;
----------------------------------------
[qnode0114 ~]$
</pre></div>
</div>
<p>If you use <code class="docutils literal notranslate"><span class="pre">salloc</span></code> instead, it will <em>not</em> automatically launch
a terminal session on the compute node. Instead, after it
schedules your job/request, it will tell you the name of the
compute node at which point you can run <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">qnodeXXXX</span></code> to
directly connect to the compute node. <em>Due to the behavior of</em>
<code class="docutils literal notranslate"><span class="pre">salloc</span></code><em>, if you lose connection to your interactive session,
the interactive job will</em><strong>not</strong><em>terminate.</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[quser21 ~]$ salloc --x11 -N 1 -n 1 --account=&lt;account&gt; --mem=&lt;XXG&gt; --partition=&lt;partition&gt; --time=&lt;hh:mm:ss&gt;
salloc: Pending job allocation 276305
salloc: job 276305 queued and waiting for resources
salloc: job 276305 has been allocated resources
salloc: Granted job allocation 276305
salloc: Waiting for resource configuration
salloc: Nodes qnode8029 are ready for job
[quser21 ~]$ ssh -X qnode8029
Warning: Permanently added 'qnode8029,172.20.134.29' (ECDSA) to the list of known hosts.
[qnode8029 ~]$
</pre></div>
</div>
<p class="rubric" id="section-job-array">Job Array</p>
<p>Job arrays can be used to submit multiple jobs at once that use
the same application script. This can be useful if you want to run
the same script multiple times with different input parameters.</p>
<p>In the example below, the –array option defines the job array,
with a specification of the index numbers you want to use (in this
case, 0 through 9). The $SLURM_ARRAY_TASK_ID bash environmental
variable takes on the value of the job array index for each job
(so here, integer values 0 through 9, one value for each job). In
this example, the value of $SLURM_ARRAY_TASK_ID is used to select
the correct index from the input_args bash array which was
constructed by reading in <em>input_args.txt</em>, each row of which is
then passed on to a script as command line arguments.</p>
<div class="highlight-filenameheader notranslate"><div class="highlight"><pre><span></span>jobsubmission.sh
</pre></div>
</div>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#SBATCH --account=w10001  ## YOUR ACCOUNT pXXXX or bXXXX
#SBATCH --partition=w10001  ### PARTITION (buyin, short, normal, w10001, etc)
#SBATCH --array=0-9 ## number of jobs to run "in parallel"
#SBATCH --nodes=1 ## how many computers do you need
#SBATCH --ntasks-per-node=1 ## how many cpus or processors do you need on each computer
#SBATCH --time=00:10:00 ## how long does this need to run (remember different partitions have restrictions on this param)
#SBATCH --mem-per-cpu=1G ## how much RAM do you need per CPU (this affects your FairShare score so be careful to not ask for more than you need))
#SBATCH --job-name="sample_job_\${SLURM_ARRAY_TASK_ID}" ## use the task id in the name of the job
#SBATCH --output=sample_job.%A_%a.out ## use the jobid (A) and the specific job index (a) to name your log file
#SBATCH --mail-type=ALL ## you can receive e-mail alerts from SLURM when your job begins and when your job finishes (completed, failed, etc)
#SBATCH --mail-user=email@u.northwestern.edu  ## your email

module purge all
module load python-anaconda3
source activate /projects/intro/envs/slurm-py37-test

IFS=$'\n' read -d '' -r -a input_args &lt; input_args.txt

python slurm_test.py --filename ${input_args[$SLURM_ARRAY_TASK_ID]}
</pre></div>
</div>
<p>where <em>input_args.txt</em> contains the following:</p>
<div class="highlight-filenameheader notranslate"><div class="highlight"><pre><span></span>input_args.txt
</pre></div>
</div>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>filename1.txt
filename2.txt
filename3.txt
filename4.txt
filename5.txt
filename6.txt
filename7.txt
filename8.txt
filename9.txt
filename10.txt
</pre></div>
</div>
<p>and <em>myscript.py</em> contains the following code:</p>
<div class="highlight-filenameheader notranslate"><div class="highlight"><pre><span></span>myscript.py
</pre></div>
</div>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>import argparse
import time

def parse_commandline():
    """Parse the arguments given on the command-line.
    """
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("--filename",
                       help="Name of file",
                       default=None)


    args = parser.parse_args()

    return args


###############################################################################
# BEGIN MAIN FUNCTION
###############################################################################
if __name__ == '__main__':
    args = parse_commandline()
    #time.sleep(10) # Sleep for 3 seconds
    print(args.filename)
</pre></div>
</div>
<p>In this example, <em>myscript.py</em> will receive the values in
<em>input.csv</em> as arguments: the first field will be sys.argv[1], the
second field will be sys.argv[2], etc.</p>
<p><strong>Note: make sure the number you specify for the –array parameter
matches the number of lines in your input file!</strong></p>
<p>Also, note that in this example standard output and error files
are printed separately for each element of the job array with the
–output and –error options. To avoid each element overwriting
these files, tag them with jobID (%A) and elementID (%a) variables
(which are automatically assigned by the scheduler) so elements
have their own distinct output and error files. An example of this
is shown in the <em>jobsubmission.sh</em> presented above.</p>
<p>Submit this script with:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>sbatch jobsubmission.sh
</pre></div>
</div>
<p>The job array will then be submitted to the scheduler with each
array element requesting the same resources (such as number of
cores, time, memory etc.) as defined in the job submission script.</p>
<p class="rubric" id="section-dependent-jobs">Dependent Jobs</p>
<p>Dependent jobs are a series of jobs which run or wait to run
conditional on the state of another job. For instance, you may
submit two jobs and you want the first job to complete
successfully before the second job runs. In order to submit this
type of workflow, you pass <strong>sbatch</strong> the jobid of the job that
needs to finish before this job starts via the command line
argument:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span><span class="n">dependency</span><span class="o">=</span><span class="n">afterok</span><span class="p">:</span><span class="o">&lt;</span><span class="n">jobid</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>To accomplish this, it is helpful to write all of your <strong>sbatch</strong>
commands in bash script. You will notice that anything you can
tell slurm via #SBATCH in the submission script itself, you can
also pass to <strong>sbatch</strong> via the command line. The key here is that
the bash variable <em>jid0, jid1, jid2</em> will contain the jobid that
SLURM assigns after you run the <strong>sbatch</strong> command.</p>
<div class="highlight-filenameheader notranslate"><div class="highlight"><pre><span></span>wrapper_script.sh
</pre></div>
</div>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
jid0=($(sbatch --time=00:10:00 --account=w10001 --partition=w10001 --nodes=1 --ntasks-per-node=1 --mem=8G --job-name=example --output=job_%A.out example_submit.sh))

echo "jid0 ${jid0[-1]}" &gt;&gt; slurm_ids

jid1=($(sbatch --dependency=afterok:${jid0[-1]} --time=00:10:00 --account=w10001 --partition=w10001 --nodes=1 --ntasks-per-node=1 --mem=8G --job-name=example --output=job_%A.out --export=DEPENDENTJOB=${jid0[-1]} example_submit.sh))

echo "jid1 ${jid1[-1]}" &gt;&gt; slurm_ids

jid2=($(sbatch --dependency=afterok:${jid1[-1]} --time=00:10:00 --account=w10001 --partition=w10001 --nodes=1 --ntasks-per-node=1 --mem=8G --job-name=example --output=job_%A.out --export=DEPENDENTJOB=${jid1[-1]} example_submit.sh))

echo "jid2 ${jid2[-1]}" &gt;&gt; slurm_ids
</pre></div>
</div>
<p>In the above, the second job will not start until the first job is
finished and the third job will not start until the second one is
finished. The actual submission script that is being run is below.</p>
<div class="highlight-filenameheader notranslate"><div class="highlight"><pre><span></span>example_submit.sh
</pre></div>
</div>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#SBATCH --mail-type=ALL ## you can receive e-mail alerts from SLURM when your job begins and when your job finishes (completed, failed, etc)
#SBATCH --mail-user=email@u.northwestern.edu ## your email

if [[ -z "${DEPENDENTJOB}" ]]; then
    echo "First job in workflow"
else
    echo "Job started after " $DEPENDENTJOB
fi

module purge all
module load python-anaconda3
source activate /projects/intro/envs/slurm-py37-test

python --version
python myscript.py --job-id $DEPENDENTJOB
</pre></div>
</div>
<p>where <em>myscript.py</em> contains the following code:</p>
<div class="highlight-filenameheader notranslate"><div class="highlight"><pre><span></span>myscript.py
</pre></div>
</div>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>import argparse
import time


def parse_commandline():
    """Parse the arguments given on the command-line.
    """
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("--job-id",
                       help="Job number",
                       default=0)

    args = parser.parse_args()

    return args


###############################################################################
# BEGIN MAIN FUNCTION
###############################################################################
if __name__ == '__main__':
    args = parse_commandline()
    time.sleep(3) # Sleep for 3 seconds
    print(args.job_id)
</pre></div>
</div>
<p>In this example, we print the job id that had to finish in order
for the dependent job to begin. Therefore, the very first job
should print 0 because it did not rely on any job to finish in
order to run but the second job should print the jobid of the
first job and so on.</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>bash wrapper_script.sh
</pre></div>
</div>
<p>This will submit the three jobs in sequence and you should see
jobs 2 and 3 pending for reason DEPENDENCY.</p>
<p class="rubric" id="section-job-scheduling">Factors Affecting Job Scheduling on Quest</p>
<p>If your job is waiting on the queue, the reason is most probably
one of the following:</p>
<ul class="simple">
<li><p>Your job’s score is lower compared to others</p></li>
<li><p>Unavailable/occupied compute resources at that moment.</p></li>
</ul>
<p class="rubric" id="priority">Priority</p>
<p>Total priority score is combination of several factors. These
factors are the following:</p>
<p><strong>i. Fair Share</strong> Quest’s job scheduler uses a fair share
mechanism to dynamically determine a score. The calculation is
based on the comparison between your share of the resources and
your actual usage of these resources. If you or other members of
your allocation used large amounts of resources in the recent
past, the priority of current jobs will be lower. Accordingly,
your jobs will wait longer before they start if the scheduler
queue is busy.</p>
<p>On the other hand, if the job queue is empty and the compute
resources are idling, regardless of the priority, your jobs will
run. You will never run out of compute hours in this model.</p>
<p>Fairshare also includes a recovery mechanism for job priority. The
contribution of past resource usage to priority calculations
decays over time. Without new usage, the job scores will be
restored significantly within a month.</p>
<p>If you are using a general access allocation, the fair share
scores of your jobs will be affected from the overall resource
usage by all members of the allocation. If you are using a buy-in
allocation that has its own compute nodes, your own usage of the
those nodes will be the determining factor for your job’s fair
share score.</p>
<p><strong>ii. Allocation Type</strong> There are two types of allocations for
research projects on Quest. Research I allocations are ideal for
small to moderate computational needs whereas Research II
allocations require considerably more resources. Due to this
difference in computational needs, a Research II allocation has a
higher share of the system resources (or initial fairshare score)
compared to a Research I allocation. This is equivalent to
receiving more compute hours with a Research II than a Research I.</p>
<p><strong>iii. Job Age</strong> Age is length of time an eligible job has been
waiting in the queue. Jobs will accumulate priority proportional
to their age. This can help overcome starting priority differences
between jobs coming from other actors.</p>
<p><strong>iv. Partition</strong> A priority score is associated with partition
that the job uses. This is only applicable for certain buy-in
allocations which have multiple partitions.</p>
<p>More detailed information about Slurm’s multi-factor priority
system, please see this
<a class="reference external" href="https://slurm.schedmd.com/priority_multifactor.html">page</a>.</p>
<p class="rubric" id="backfill-scheduling">Backfill Scheduling</p>
<p>There is a secondary mechanism that starts lower priority jobs on
slots reserved by higher priority jobs while these jobs are
acquiring their full set of resources. This is called “backfill”
scheduling which helps to increase the utilization of the compute
nodes and guarantees no delay in starting the higher priority
jobs. To benefit from this mechanism, it is important to
accurately request resources (wall time, core, memory) so that the
scheduler can find appropriate space on the resource map. Please
review <a class="reference external" href="https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1695">resource utilization
page</a>
for different methods you can use to identify your job’s needs.</p>
<p>From time to time, compute resources cannot be backfilled
effectively and nodes/cores may appear idle while jobs are waiting
on the queue.</p>
<p class="rubric" id="section-diagnosing-jobs">Diagnosing Issues with Your Job Submission Script
and/or Your Job Itself</p>
<p class="rubric" id="section-debugging-jobs">Debugging a Job Submission Script Rejected By The
Scheduler</p>
<p>If your job submission script generates an error when you submit
it with the <strong>sbatch</strong> command, the problem in your script is in
one or more of the lines that begin with #SBATCH. To debug job
scripts that generate errors, look up the error message in the
section below to identify the most likely reason your script
received that error message. Once you have identified the mistake
in your script, edit your script to correct it and re-submit your
job. If you receive the same error message again, examine the
error message and the mistake in your script more closely.
Sometimes the same error message can be generated by two different
mistakes in the same script, meaning it is possible that you may
resolve the first mistake but need to correct a second mistake to
clear that particular error message. Mistakes can be difficult to
identify, and often require careful reading of your #SBATCH lines.</p>
<p>When you re-submit your job you may receive a new error message.
This means the mistake that generated the first error message has
been resolved, and now you need to fix another mistake. Slurm
returns up to two distinct error messages at a time. If your
submission script has more than two mistakes, you will need to
re-submit your job multiple times to identify and fix all of them.</p>
<p>When Slurm encounters a mistake in your job submission script, it
does not read the rest of your script that comes after the
mistake. If the mistake generates an error, you can fix it and
resubmit your job, however not all mistakes generate errors. If
your script’s required elements (account, partition, nodes, cores,
and wall time) have been read successfully before Slurm encounters
your mistake, your job will still be accepted by the scheduler and
run, just not the way you expect it to. Scripts with mistakes that
don’t generate errors still need to be debugged since the
scheduler has ignored some of your #SBATCH lines. You can identify
a script with mistakes if the output from your job is unexpected
or incorrect.</p>
<p>To use the references below, search for the exact error message
generated by your job. Some error messages appear to be similar
but are generated by different mistakes.</p>
<p>Note that the errors listed in this document may also be generated
by interactive job submissions using <code class="docutils literal notranslate"><span class="pre">srun</span></code> or <code class="docutils literal notranslate"><span class="pre">salloc</span></code>. In
those cases, the error messages will begin with <code class="docutils literal notranslate"><span class="pre">srun</span></code> error or
<code class="docutils literal notranslate"><span class="pre">salloc</span></code> error. The procedure to resolve these error messages is
the same.</p>
<p>With certain combinations of GUI editors and character sets on
your personal computer, copying and pasting into Quest job
submission scripts may bring in specific hidden characters that
interfere with the scheduler’s ability to interpret the script. In
these cases, #SBATCH lines will have no mistakes but still
generate errors when submitted to the scheduler. To see all of the
hidden characters in your job submission script, use the command
cat -A &lt;script_name&gt;. To resolve this, you may need to type your
submission script into a native unix editor like vi and not use
copy and paste.</p>
<div class="panel panel-default docutils container">
<div class="panel-heading docutils container">
<p>sbatch: error: –account option required or sbatch: error:
Unable to allocate resources: Invalid account or
account/partition combination specified</p>
</div>
<p>panel panel-body js-panelnormalswitches0 collapse</p>
<blockquote>
<div><div class="line-block">
<div class="line">Location of mistake:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--account=&lt;allocation&gt;</span></code></div>
<div class="line">or</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-A</span> <span class="pre">&lt;allocation&gt;</span></code></div>
<div class="line">Example of correct account syntax:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--account=p12345</span></code></div>
<div class="line">or</div>
<div class="line">#SBATCH -A p12345</div>
</div>
<div class="line-block">
<div class="line">Possible mistake: your script doesn’t have an <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code>
line specifying account</div>
<div class="line">Fix: confirm that <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--account=&lt;allocation&gt;</span></code> is in
your script.</div>
</div>
<div class="line-block">
<div class="line">Possible mistake: a typo in the “–account=” or “-A” part
of this <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> line</div>
<div class="line">Fix: examine this line closely to make sure the syntax is
correct</div>
</div>
<div class="line-block">
<div class="line">Possible mistake: you are not a member of the allocation
specified in your job submission script</div>
<div class="line">Fix: confirm you are a member of the allocation by typing
<code class="docutils literal notranslate"><span class="pre">groups</span></code> at the command line on Quest. If the allocation
you have specified in your job submission script is not
listed, you are not a member of this allocation. Use an
allocation that you are a member of in your job submission
script.</div>
</div>
<div class="line-block">
<div class="line">Possible mistake: the mistake is on a line earlier in your
job submission script which causes Slurm to stop reading
your script before it reaches the
<code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--account=&lt;allocation&gt;</span></code> line</div>
<div class="line">Fix: Move the <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--account=&lt;allocation&gt;</span></code> line to
be immediately after the line <code class="docutils literal notranslate"><span class="pre">#!/bin/bash</span></code> and submit
your job again. If this generates a new error referencing
a different line of your script, the account line is
correct and the mistake is elsewhere in your submission
script. To resolve the new error, follow the debugging
suggestions for the new error message.</div>
</div>
</div></blockquote>
</div>
<div class="panel panel-default docutils container">
<div class="panel-heading docutils container">
<p>sbatch: error: Your allocation has expired or sbatch: error:
Unable to allocate resources: Invalid qos specification</p>
</div>
<p>panel panel-body js-panelnormalswitches1 collapse</p>
<blockquote>
<div><div class="line-block">
<div class="line">Location of mistake:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--account=&lt;allocation&gt;</span></code></div>
<div class="line">or</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-A</span> <span class="pre">&lt;allocation&gt;</span></code></div>
</div>
<p>The allocation specified in your job submission script is no
longer active.</p>
<p>If you are a member of more than one allocation, you may
wish to submit your job to an alternate allocation. To see a
list your allocations, type <code class="docutils literal notranslate"><span class="pre">groups</span></code> at the command line
on Quest.</p>
<p>To renew your allocation or request a new one, please see
<a class="reference external" href="https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1486">Managing an Allocation on
Quest</a>.</p>
</div></blockquote>
</div>
<div class="panel panel-default docutils container">
<div class="panel-heading docutils container">
<p>srun: error: –partition option required or srun: error:
Unable to allocate resources: Access/permission denied</p>
</div>
<p>panel panel-body js-panelnormalswitches2 collapse</p>
<blockquote>
<div><div class="line-block">
<div class="line">Location of mistake:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--partition=&lt;partition/queue&gt;</span></code></div>
<div class="line">or</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-p</span> <span class="pre">&lt;partition/queue&gt;</span></code></div>
</div>
<div class="line-block">
<div class="line">Example of correct syntax for general access allocations
(“p” account):</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--partition=short</span></code></div>
<div class="line">or</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-p</span> <span class="pre">short</span></code></div>
</div>
<div class="line-block">
<div class="line">Example of correct syntax for buy-in allocations (“b”
account):</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--partition=buyin</span></code></div>
<div class="line">or</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-p</span> <span class="pre">buyin</span></code></div>
</div>
<p>Note that Slurm refers to queues as partitions.</p>
<div class="line-block">
<div class="line">Possible mistake: your script doesn’t have an <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code>
line specifying partition</div>
<div class="line">Fix: confirm that
<code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--partition=&lt;partition/queue&gt;</span></code> or
<code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-p</span> <span class="pre">&lt;partition/queue&gt;</span></code> is in your script.</div>
</div>
<div class="line-block">
<div class="line">Possible mistake: a typo in the “–partition=” or “-p” part
of this <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> line</div>
<div class="line">Fix: examine this line closely to make sure the syntax is
correct</div>
</div>
<div class="line-block">
<div class="line">Possible mistake: the mistake is on a line earlier in your
job submission script which causes Slurm to stop reading
your script before it reaches the
<code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--account=&lt;allocation&gt;</span></code> line</div>
<div class="line">Fix: Move the <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--account=&lt;allocation&gt;</span></code> line to
be immediately after the line <code class="docutils literal notranslate"><span class="pre">#!/bin/bash</span></code> and submit
your job again. If this generates a new error referencing
a different line of your script, the account line is
correct and the mistake is elsewhere in your submission
script. To resolve the new error, follow the debugging
suggestions for the new error message.</div>
</div>
</div></blockquote>
</div>
<div class="panel panel-default docutils container">
<div class="panel-heading docutils container">
<p>sbatch: error: Unable to allocate resources: Invalid qos
specification</p>
</div>
<p>panel panel-body js-panelnormalswitches3 collapse</p>
<blockquote>
<div><div class="line-block">
<div class="line">Location of mistake:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--partition=&lt;partition/queue&gt;</span></code></div>
<div class="line">or</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-p</span> <span class="pre">&lt;partition/queue&gt;</span></code></div>
</div>
<p>The partition/queue name specified is not associated with
the allocation in the line
<code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--account=&lt;allocation&gt;</span></code>.</p>
<p>Possible mistake: Your script specifies a buy-in allocation,
and you’ve specified “short”, “normal” or “long” as your
partition/queue.</p>
<div class="line-block">
<div class="line">Possible mistake: Your script specifies an allocation and
partition combination which do not belong together.</div>
<div class="line">Fix: Specify the correct partition/queue for your
allocation. To see the allocations and partitions you have
access to, use this version of the <code class="docutils literal notranslate"><span class="pre">sinfo</span></code> command:</div>
</div>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>sinfo -o "%g %.10R %.20l"
GROUPS      PARTITION         TIMELIMIT
b1234       buyin             168:00:00
</pre></div>
</div>
<p>Note that “GROUPS” are allocations/accounts on Quest.
In this example, valid lines in your job submission script
that relate to account, partition and time would be:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>#SBATCH --account=b1234
#SBATCH --partition=buyin
#SBATCH --time=168:00:00
</pre></div>
</div>
</div></blockquote>
</div>
<div class="panel panel-default docutils container">
<div class="panel-heading docutils container">
<p>sbatch: error: invalid partition specified: &lt;partition_name&gt;
or sbatch: error: Unable to allocate resources: Invalid
partition name specified</p>
</div>
<p>panel panel-body js-panelnormalswitches4 collapse</p>
<blockquote>
<div><div class="line-block">
<div class="line">Location of mistake:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--partition=&lt;partition/queue&gt;</span></code></div>
<div class="line">or</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-p</span> <span class="pre">&lt;partition/queue&gt;</span></code></div>
</div>
<div class="line-block">
<div class="line">Example of correct syntax for general access allocations
(“p” account):</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--partition=short</span></code></div>
<div class="line">or</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-p</span> <span class="pre">short</span></code></div>
</div>
<div class="line-block">
<div class="line">Example of correct syntax for buy-in allocations (“b”
account):</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--partition=buyin</span></code></div>
<div class="line">or</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-p</span> <span class="pre">buyin</span></code></div>
</div>
<div class="line-block">
<div class="line">Possible mistake: a typo in the “–partition=” or “-p” part
of this <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> line</div>
<div class="line">Fix: examine this line closely to make sure the syntax is
correct</div>
</div>
<div class="line-block">
<div class="line">Possible mistake: Your script specifies a general access
allocation (“p” account) with a queue that isn’t “short”,
“normal” or “long”.</div>
<div class="line">Fix: change your partition to be “short”, “normal” or
“long”</div>
</div>
</div></blockquote>
</div>
<div class="panel panel-default docutils container">
<div class="panel-heading docutils container">
<p>sbatch: error: Unable to allocate resources: Invalid account
or account/partition combination specified or sbatch: error:
Unable to allocate resources: User’s group not permitted to
use this partition</p>
</div>
<p>panel panel-body js-panelnormalswitches5 collapse</p>
<blockquote>
<div><p>This message can refer to mistakes on the <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> lines
specifying account or partition.</p>
<div class="line-block">
<div class="line">Possible location of mistake specifying account:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--account=&lt;allocation&gt;</span></code></div>
<div class="line">or</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-A</span> <span class="pre">&lt;allocation&gt;</span></code></div>
</div>
<div class="line-block">
<div class="line">Possible location of mistake specifying partition</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--partition=&lt;partition/queue&gt;</span></code></div>
<div class="line">or</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-p</span> <span class="pre">&lt;partition/queue&gt;</span></code></div>
</div>
<div class="line-block">
<div class="line">Possible mistake: the syntax in the #SBATCH line
specifying account is incorrect</div>
<div class="line">Fix: examine the account line closely to confirm the
syntax is exactly correct. Example of correct account
syntax:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--account=p12345</span></code></div>
<div class="line">or</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-A</span> <span class="pre">p12345</span></code></div>
</div>
<div class="line-block">
<div class="line">Possible mistake: you are trying to run in a
partition/queue that belongs to one account, while
specifying a different account.</div>
<div class="line">Fix: Specify the correct partition/queue for your
allocation. To see the allocations and partitions you have
access to, use this version of the <code class="docutils literal notranslate"><span class="pre">sinfo</span></code> command:</div>
</div>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>sinfo -o "%g %.10R %.20l"
GROUPS      PARTITION         TIMELIMIT
b1234       buyin             168:00:00
</pre></div>
</div>
<p>Note that “GROUPS” are allocations/accounts on Quest.
In this example, valid lines in your job submission script
that relate to account, partition and time would be:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>#SBATCH --account=b1234
#SBATCH --partition=buyin
#SBATCH --time=168:00:00
</pre></div>
</div>
<div class="line-block">
<div class="line">Possible mistake: the mistake is on a line earlier in your
job submission script which causes Slurm to stop reading
your script before it reaches the
<code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--account=&lt;allocation&gt;</span></code> line</div>
<div class="line">Fix: Move the <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--account=&lt;allocation&gt;</span></code> line to
be immediately after the line <code class="docutils literal notranslate"><span class="pre">#!/bin/bash</span></code> and submit
your job again. If this generates a new error referencing
a different line of your script, the account line is
correct and the mistake is elsewhere in your submission
script. To resolve the new error, follow the debugging
suggestions for the new error message.</div>
</div>
</div></blockquote>
</div>
<div class="panel panel-default docutils container">
<div class="panel-heading docutils container">
<p>sbatch: error: –time limit option required or sbatch: error:
Unable to allocate resources: Requested time limit is
invalid (missing or exceeds some limit)</p>
</div>
<p>panel panel-body js-panelnormalswitches6 collapse</p>
<blockquote>
<div><div class="line-block">
<div class="line">Location of mistake:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--time=&lt;hours:minutes:seconds&gt;</span></code></div>
<div class="line">or</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-t</span> <span class="pre">&lt;hours:minutes:seconds&gt;</span></code></div>
</div>
<div class="line-block">
<div class="line">Example of correct syntax:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--time=10:00:00</span></code></div>
<div class="line">or</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-t</span> <span class="pre">10:00:00</span></code></div>
</div>
<div class="line-block">
<div class="line">Possible mistake: your script doesn’t have an <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code>
line specifying time</div>
<div class="line">Fix: confirm that <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--time=&lt;hh:mm:ss&gt;</span></code> is in your
script.</div>
</div>
<div class="line-block">
<div class="line">Possible mistake: a typo in the “–time=” or “-t” part of
this <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> line</div>
<div class="line">Fix: examine this line closely to make sure the syntax is
correct.</div>
</div>
<div class="line-block">
<div class="line">Possible mistake: the time request is too long for the
partition (queue)</div>
<div class="line">Fix: review the wall time limits of your partition and
adjust the amount of time requested by your script. For
general access users with allocations that begin with a
“p”, please use this reference:</div>
</div>
<div class="table-responsive docutils container">
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Partition</p></th>
<th class="head"><p>Walltime limit</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>short</p></td>
<td><p>4 hours</p></td>
</tr>
<tr class="row-odd"><td><p>normal</p></td>
<td><p>48 hours</p></td>
</tr>
<tr class="row-even"><td><p>long</p></td>
<td><p>7 days / 168 hours</p></td>
</tr>
<tr class="row-odd"><td><p>genhimem</p></td>
<td><p>48 hours</p></td>
</tr>
<tr class="row-even"><td><p>gengpu</p></td>
<td><p>48 hours</p></td>
</tr>
</tbody>
</table>
</div>
<p>Buy-in accounts that begin with a “b” have their own wall
time limits. For information on the wall time of your
partition, use the <code class="docutils literal notranslate"><span class="pre">sinfo</span></code> command:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>sinfo -o "%g %.10R %.20l"
GROUPS      PARTITION         TIMELIMIT
b1234       buyin             168:00:00
</pre></div>
</div>
<p>To fix this error, set your wall time to be less than the
time limit of your partition and re-submit your job.</p>
<div class="line-block">
<div class="line">Possible mistake: the mistake is on a line earlier in your
job submission script which causes Slurm to stop reading
your script before it reaches the
<code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--account=&lt;allocation&gt;</span></code> line</div>
<div class="line">Fix: Move the <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--time=&lt;hh:mm::ss&gt;</span></code> line to be
immediately after the line #!/bin/bash and submit your job
again. If this generates a new error referencing a
different line of your script, the account line is correct
and the mistake is elsewhere in your submission script. To
resolve the new error, follow the debugging suggestions
for the new error message.</div>
</div>
</div></blockquote>
</div>
<div class="panel panel-default docutils container">
<div class="panel-heading docutils container">
<p>sbatch: unrecognized option &lt;option&gt;</p>
</div>
<p>panel panel-body js-panelnormalswitches7 collapse</p>
<blockquote>
<div><div class="line-block">
<div class="line">Example:</div>
<div class="line">Line in script: <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--n-tasks-per-node=1</span></code></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Error generated sbatch: unrecognized option ‘--n-tasks-per-node=1'
</pre></div>
</div>
<p>With an “unrecognized option” error, Slurm correctly read
the first part of the <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> line but the option that
follows it has generated the error. In this example, the
option has a dash between “n” and “tasks” that should not be
there. The correct option does not have a dash in that
location. This line should be corrected to:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --ntasks-per-node=1</span>
</pre></div>
</div>
<p>To fix this error, locate the option specified in the error
message and examine it carefully for errors. To see correct
syntax for all <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> directives, see <a class="reference external" href="https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1795">Converting
Moab/Torque scripts to
Slurm</a>.</p>
</div></blockquote>
</div>
<div class="panel panel-default docutils container">
<div class="panel-heading docutils container">
<p>sbatch: error: CPU count per node can not be satisfied or
sbatch: error: Batch job submission failed: Requested node
configuration is not available</p>
</div>
<p>panel panel-body js-panelnormalswitches8 collapse</p>
<blockquote>
<div><div class="line-block">
<div class="line">Location of mistake:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--ntasks-per-node=&lt;CPU</span> <span class="pre">count&gt;</span></code></div>
<div class="line">Example of mistake:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--ntasks-per-node=10000</span></code></div>
</div>
<p>This error is generated if your job requests more CPUs/cores
than are available on the nodes in the partition your job
submission script specified. CPU count is the number of
cores requested by your job submission script. Cores are
also called processors or CPUs.</p>
<p>To fix this mistake, use the <code class="docutils literal notranslate"><span class="pre">sinfo</span></code> command to get the
maximum number of cores available in the partitions you have
access to:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>sinfo -o "%g %.10R %.20l %.10c"
GROUPS      PARTITION       TIMELIMIT       CPUS
b1234       buyin           2-00:00:00      20+
</pre></div>
</div>
<p>In this example, your job submission script can request up
to 20 CPUs/cores per node like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --ntasks-per-node=20</span>
</pre></div>
</div>
</div></blockquote>
</div>
<div class="panel panel-default docutils container">
<div class="panel-heading docutils container">
<p>sbatch: error: Batch script contains DOS line breaks (rn)
or sbatch: error: instead of expected UNIX line breaks (n).</p>
</div>
<p>panel panel-body js-panelnormalswitches9 collapse</p>
<blockquote>
<div><p>Location of mistake:</p>
<p>Hidden characters in your job submission script</p>
<p>Mistake: your job submission script was created on a Windows
machine and copied onto Quest without converting it into
UNIX encoded characters.</p>
<p>Fix: from the command line on Quest run the command
<code class="docutils literal notranslate"><span class="pre">dos2unix</span> <span class="pre">&lt;submission_script</span></code>&gt; to correct your job
submission script and re-submit your job to the scheduler.</p>
</div></blockquote>
</div>
<p class="rubric" id="h-94310085932911650907024000">Debugging a Job Accepted by the Scheduler</p>
<p>Once your job has been accepted, the Slurm scheduler will return a
job id number. After waiting in the queue, your job will run. To
see the status of your job, use the command <code class="docutils literal notranslate"><span class="pre">sacct</span> <span class="pre">-X</span></code>.</p>
<p>For jobs with mistakes that do not give error messages, you will
need to investigate if you notice something is wrong with how the
job runs. If you notice a problem on the list below, click on it
for debugging suggestions.</p>
<div class="panel panel-default docutils container">
<div class="panel-heading docutils container">
<p>Job runs very slowly or dies after starting</p>
</div>
<p>panel panel-body js-panelnormalswitches10 collapse</p>
<blockquote>
<div><div class="line-block">
<div class="line">Problem: job runs very slowly, or dies after starting</div>
<div class="line">Possible cause: job script is not reading the directive
<code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--mem=&lt;amount&gt;</span></code>.</div>
</div>
<p>All Slurm job scripts should specify the amount of memory
your job needs to run. If your job runs very slowly or dies,
investigate if it requests enough memory with the Slurm
utility <code class="docutils literal notranslate"><span class="pre">seff</span></code>. For more information, see <a class="reference external" href="https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1695">Checking
Processor and Memory Utilization for Jobs on
Quest</a>.</p>
</div></blockquote>
</div>
<div class="panel panel-default docutils container">
<div class="panel-heading docutils container">
<p>Job name is name of job submission script instead of name in
submission script</p>
</div>
<p>panel panel-body js-panelnormalswitches11 collapse</p>
<blockquote>
<div><div class="line-block">
<div class="line">Problem: job name is name of job submission script instead
of name in submission script</div>
<div class="line">Possible cause: job script is not reading the
<code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--job-name=&lt;job</span> <span class="pre">name&gt;</span></code> directive.</div>
</div>
<div class="line-block">
<div class="line">Slurm is not reading the <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> directive:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">-J</span> <span class="pre">&lt;Job_Name&gt;</span></code></div>
<div class="line">or</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--job-name=&lt;Job_Name&gt;</span></code></div>
</div>
<p>To see the name of your job, run <code class="docutils literal notranslate"><span class="pre">sacct</span> <span class="pre">-X</span></code>. If JOB NAME
is the first eight characters of the name of your submission
script, SLURM has not read the <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> lines for job
name.</p>
<div class="line-block">
<div class="line">Possible Mistake: a typo in the “–job-name=” or “-J” part
of this <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> line</div>
<div class="line">Fix: examine this line closely to make sure the syntax is
correct</div>
</div>
<div class="line-block">
<div class="line">Possible mistake: the mistake is on a line earlier in your
job submission script which causes Slurm to stop reading
your script before it reaches the
<code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--job-name=&lt;job</span> <span class="pre">name&gt;</span></code> line</div>
<div class="line">Fix: Move the <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--job-name=&lt;job</span> <span class="pre">name&gt;</span></code> line to be
immediately after the line <code class="docutils literal notranslate"><span class="pre">#!/bin/bash</span></code> and submit your
job again. If this generates a new error referencing a
different line of your script, the account line is correct
and the mistake is elsewhere in your submission script. To
resolve the new error, follow the debugging suggestions
for the new error message.</div>
</div>
</div></blockquote>
</div>
<div class="panel panel-default docutils container">
<div class="panel-heading docutils container">
<p>Modules or environment variables are inherited from the
login session by a running job</p>
</div>
<p>panel panel-body js-panelnormalswitches12 collapse</p>
<blockquote>
<div><div class="line-block">
<div class="line">Problem: modules or environmental variables are inherited
from the login session by a running job</div>
<div class="line">Possible cause: job script is not purging modules before
beginning compute node session</div>
</div>
<p>Fix: after the <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> directives in your job submission
script, add the line</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>module purge all
</pre></div>
</div>
<p>This will clear any modules inherited from your login
session, and begin your job in a clean environment. You will
need to load any necessary modules into your job submission
script after this line.</p>
<p class="rubric" id="job-immediately-fails-and-generates-no-output-or-error-file">Job immediately fails and generates no output or
error file</p>
<div class="line-block">
<div class="line">Problem: job can’t write into output and/or error files so
job immediately dies</div>
<div class="line">Possible cause: job script specifies directory path for
output and/or error files but does not provide a file name</div>
<div class="line">Possible cause: job script specifies a directory that does
not exist</div>
</div>
<p>Slurm is not getting a file name that it can write into in
the SBATCH directive:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>#SBATCH –-output=/path/to/file/file_name
</pre></div>
</div>
<p>or</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>#SBATCH --error=/path/to/file/file_name
</pre></div>
</div>
<div class="line-block">
<div class="line">Possible Mistake: a typo in the “–output=” or “–error”
part of this #SBATCH line</div>
<div class="line">Fix: examine this line closely to make sure the syntax is
correct</div>
</div>
<div class="line-block">
<div class="line">Possible Mistake: providing a directory but not a file
name for output and/or error files</div>
<div class="line">Fix: add a file name at the end of the specified path. For
a file name in the format <code class="docutils literal notranslate"><span class="pre">&lt;job_name&gt;.o&lt;job_id&gt;</span></code>, use</div>
</div>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>#SBATCH –-output=/path/to/file/"%x.o%j"
</pre></div>
</div>
<p>Note if a separate error file is not specified, errors and
output will both be written into the output file. To
generate a separate error file, include the line:</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>#SBATCH –-error=/path/to/file/"%x.e%j"
</pre></div>
</div>
</div></blockquote>
</div>
<p class="rubric" id="section-job-failures">Common reasons for Failed Jobs</p>
<p>This section provides some common reasons for why your job may
fail and how to go about fixing it.
.. rubric:: Job Exceeded Request Time or Memory</p>
<blockquote>
<div><dl class="field-list simple">
<dt class="field-odd">name<span class="colon">:</span></dt>
<dd class="field-odd"><p>#section-ran-out-of-disk</p>
</dd>
</dl>
</div></blockquote>
<p>Besides errors in your script or hardware failure, your job may be
aborted by the system if it is still running when the walltime
limit you requested (or the upper walltime limit for the
partition) is reached. You will see <code class="docutils literal notranslate"><span class="pre">TIMEOUT</span></code> state for these
jobs.</p>
<p>If you use more cores than you requested, the system will again
stop the job. This can happen with programs that are
multi-threaded. Similarly, if the job exceeds the requested
memory, the job will be terminated. Due to this, it is important
to profile your code for the memory requirement.</p>
<p>If you do not set the number of nodes/cores, memory or time in
your job submission script, the default values will be assigned by
the scheduler.</p>
<p class="rubric" id="section-ran-out-of-disk">Out of Disk Space</p>
<p>Your job could also fail if you exceed your storage quote in your
home or projects directory.</p>
<p>Check how much space you are using in your home directory with</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>homedu
</pre></div>
</div>
<p>or</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>du -h --max-depth=0 ~
</pre></div>
</div>
<p>Check how much space is used in your projects directory with</p>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>checkproject &lt;allocationID&gt;
</pre></div>
</div>
</div>
</div>
</section>


          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
            <a href="../Disconnecting-from-a-RDSS-FSMResFiles-share/Disconnecting-from-a-RDSS-FSMResFiles-share-updated.html" title="Disconnecting from a RDSS/FSMResFiles share"
               class="md-flex md-footer-nav__link md-footer-nav__link--prev"
               rel="prev">
              <div class="md-flex__cell md-flex__cell--shrink">
                <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
              </div>
              <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
                <span class="md-flex__ellipsis">
                  <span
                      class="md-footer-nav__direction"> Previous </span> Disconnecting from a RDSS/FSMResFiles share </span>
              </div>
            </a>
          
          
            <a href="../Examples-of-Jobs-on-Quest/Examples-of-Jobs-on-Quest-updated.html" title="Examples of Jobs on Quest"
               class="md-flex md-footer-nav__link md-footer-nav__link--next"
               rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span
                class="md-flex__ellipsis"> <span
                class="md-footer-nav__direction"> Next </span> Examples of Jobs on Quest </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink"><i
                class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2023, Quest.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 6.1.3.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="../_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>